{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a7337dac0fd4b93bc4f20d637f7e3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6aa5661e80f42118dd2b4cd0ea6f970",
              "IPY_MODEL_c29da84cb54949b0bd20bf418ced96b6",
              "IPY_MODEL_8f5736185bf143f18fa2fdf046faad30"
            ],
            "layout": "IPY_MODEL_d5ab7d0717fb484cbcbe18a9fa285752"
          }
        },
        "c6aa5661e80f42118dd2b4cd0ea6f970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_492ffdadb4c84d4080e6421003a12eac",
            "placeholder": "​",
            "style": "IPY_MODEL_22fb60f2a9064bfe98176d957b5cf9e5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c29da84cb54949b0bd20bf418ced96b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2be7420baac1479293df651e14f222d4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aaa9eeae5ac24979ab5dc62cedaefb6f",
            "value": 2
          }
        },
        "8f5736185bf143f18fa2fdf046faad30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03b090cdeb094bf3b74494fb8f75c61c",
            "placeholder": "​",
            "style": "IPY_MODEL_8843c99bea2f4ca4a3c51262c4f6e13e",
            "value": " 2/2 [01:08&lt;00:00, 31.52s/it]"
          }
        },
        "d5ab7d0717fb484cbcbe18a9fa285752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492ffdadb4c84d4080e6421003a12eac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22fb60f2a9064bfe98176d957b5cf9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2be7420baac1479293df651e14f222d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaa9eeae5ac24979ab5dc62cedaefb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03b090cdeb094bf3b74494fb8f75c61c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8843c99bea2f4ca4a3c51262c4f6e13e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waghmareps12/RANDOM_COLLAB_LLM_NOTEBOOKS/blob/main/Implementing_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0LynaFxGFLw",
        "outputId": "cef1f505-1b02-4785-c166-4326cb3b2724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.22.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.13)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.24)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.110.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.13.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.3.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.15)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.13.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.29)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.33)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.31)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.1.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.5.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.17.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.44b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (29.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.36.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.23.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.44b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.21.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch accelerate gradio langchain chromadb sentence_transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "lmzDFb_VG9P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Document**"
      ],
      "metadata": {
        "id": "vgLAOQZ5IafK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader('/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt')\n",
        "documents = loader.load()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdoEHxKsIS9H",
        "outputId": "03879e9e-f0fd-4b91-cf85-cec3704a1050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"\\ufeffSentiment Analysis of Social Media Text: Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions\\nEddy Ejembi\\neddyejembi2018@gmail.com\\n\\n\\nAbstract\\nThe emergence of social media has provided a platform for individuals to express a wide range of sentiments and emotions. Sentiment analysis, the task of determining the emotional tone behind text data, has gained prominence for its relevance in various domains. This research project aims to address a notable gap in existing sentiment analysis systems by focusing on sentiments that are often overlooked, including depression, suicidal thoughts, feelings of threat, fear, and other emotionally charged states. The project utilizes fine-tuned language models to achieve more accurate and comprehensive sentiment analysis in the context of social media. Through data collection, preprocessing, fine-tuning, and evaluation, the research contributes to the improvement of sentiment analysis for a broader range of emotions and states expressed on social media.\\nKeywords: Sentiment analysis, social media, fine-tuned language models, emotional states, mental health\\n\\n\\n________________\\nIntroduction\\nSentiment analysis, also known as opinion mining, has established itself as a valuable tool for understanding the emotional content present in text data. This field of research and application has been widely adopted in various domains, such as marketing, customer service, politics, and public opinion analysis. Existing sentiment analysis systems primarily focus on categorizing text into common categories: positive, negative, and neutral (Wankhade et al., 2022; Sutar et al., 2016; Kakde & Losarwar, 2018; Gah et al., 2017). These systems are instrumental in understanding the general sentiment of social media content, which often pertains to subjects like product reviews, political discourse, and general public sentiment.\\nSocial media has become an integral part of modern communication, providing a platform for individuals to connect, share information, and engage in various forms of expression. Users utilize social media to disseminate news, engage in discussions, and express their opinions on a wide range of topics. However, social media's role extends beyond mere information exchange, as it has also evolved into a space for self-expression and casual emotional release. Many individuals utilize social media to share their personal experiences, both positive and negative, including moments of joy, sadness, and even depression. This openness and willingness to share personal narratives have created a unique social landscape where individuals can connect with others on an emotional level. Studies have demonstrated that people increasingly turn to online platforms like social media to share their moods, thoughts and feelings (Choudhury, 2013; Prieto et al., 2014; Park et al., 2012; Jagadishwari et al., 2021).\\nSentiment analysis has proven its importance for brand monitoring, public opinion analysis, and content moderation on social media platforms. Businesses employ sentiment analysis to gauge customer satisfaction, while policymakers and researchers utilize it to access public opinion trends. The ability to capture and analyze general public sentiment is very valuable for social events, political movements, marketing campaigns, product preferences, and the business world (Cambria et al., 2017). Understanding public sentiments on social media plays a crucial role in elections worldwide (Mohammad et al., 2015). Business leaders analyze the holistic view of people on social media and other platforms, and use this information to track feelings and opinions with respect to their product (Gaind et al., 2023).\\nSentiment analysis has demonstrated significant importance in business, politics, and social events; it can also play a crucial role in public health monitoring particularly in mental and emotional health. Many researchers claim that social media analysis is a very helpful source in various contexts especially in mental health understanding (Martínez-Castaño et al., 2020). Individuals often turn to social media to express themselves through posts detailing their depression or fear, which may lead to self harm, or harm initiated by a second actor. According to (Losada et al., 2017), early detection and diagnosis of risk like self harm, depression, suicide, and others, especially those initiated by single actors, are critically important and pose a challenging and increasingly important research area for individuals, and public health.\\nNumerous studies on the mental health of users have been conducted through social media data collection and analysis, either through surveys requiring questionnaires or by scraping data from public posts using keywords, phrases, and other techniques. (Orabi et al., 2018), employed a deep neural network to analyze the Twitter database for depression diagnosis. (Choudhury et al., 2021), analyzed Twitter social media text for public health prediction, and examined how social media has been a valuable tool in predicting and detecting affective disorder in individuals.\\nTraditionally, sentiment analysis has been classified into two types: binary classification, and ternary classification. In binary classification, sentiments are classified into polarities or classes of two: Positive and Negative (Tanna et al., 2020). Ternary classification on the other hand, classifies sentiments into three classes; Positive, Negative and Neutral (Arora & Arora, 2019; Chen et al., 2018). Ternary classification tends to produce more classification error than binary classification. (Gaind et al., 2023) solves the problem of most Sentiment analysis which classify text into binary or ternary sentiments by proposing a more profound way of classifying, and quantifying text according to six standard emotions suggested by Paul Ekman (Ekman, 1992). (Jabreel & Moreno, 2019) also proposed a novel transformation approach called xy-pair-set for classifying multi-labels emotions on tweets.\\nHowever, the existing sentiment analysis systems often overlook sentiments that are of profound significance, particularly those expressed by individuals who may be in emotional distress. Emotions such as depression, suicidal thoughts, feelings of threat, fear, and other psychologically charged states are not always well-addressed by traditional sentiment analysis tools. This omission is a notable gap that requires attention.\\nThis research project aims to fill the identified gap by utilizing fine-tuned language models to enhance sentiment analysis on social media. The project hypothesizes that fine-tuning a language model will result in improved sentiment analysis by allowing the model to identify and classify emotions that are frequently associated with psychological distress. The value of filling this gap is substantial, as it will enable more accurate identification and tracking of sentiments indicative of emotional well-being or distress in the online community.\\n\\n\\nMaterials and Methods\\nThis research project adopts a fine-tuning approach using a state-of-the-art language model, GPT-3.5-turbo, as the basis for sentiment analysis. Fine-tuning is a process that allows a pre-trained language model to adapt to a specific task or domain, making it particularly well-suited for addressing the research aims. GPT-3.5 turbo is a Large Language Chat Model rooted in the GPT 3.5 architecture. This model powers the ChatGPT platform and is distinguished by its substantial capacity, featuring an impressive 175 billion parameters. It has been meticulously developed using a vast corpus of real-world textual data.\\nThe adoption of GPT-3.5-turbo aligns seamlessly with the pursuit of advancing sentiment analysis on social media, equipping us with a versatile and potent tool that holds immense promise for understanding and categorizing a broad spectrum of emotions and expressions within the digital realm.\\nData Collection\\nData for fine-tuning the sentiment analysis model is a Twitter dataset obtained from Kaggle. Twitter is a prominent social media platform known for its diverse and real-time content. The data collection process focuses on identifying tweets that contain expressions of sadness, happiness, feelings of threat, fear, neutral feeling and other relevant emotions. The dataset contains forty thousand tweets with three columns. The first column contains the tweet ID, the second column is the tweet sentiment which is the emotion behind the tweet, while the third column contains the actual tweet. This dataset has thirteen distinct emotions, which given the extensive scope of the pre-trained language model serving as its foundation, is considered adequate.\\n  \\n\\nFigure 1. Dataset Sample\\nThe collected data undergoes preprocessing which includes converting from a csv to a jsonl file and preparing in the format required to finetune GPT-3.5-turbo.\\nDuring the fine-tuning process in line with the requirements of GPT-based models built on the Transformer architecture, the data underwent essential steps, including text cleaning, tokenization, and handling of special characters. Tokenization is the method of segmenting text data into smaller units called tokens. These tokens, which can represent words or subwords, are fundamental for processing text with machine learning models. It is a key technique in the transformer architecture, enabling language understanding and accommodating a broad vocabulary, even for previously unseen words (Vaswani et al., 2017)\\n  \\n\\nFigure 2. Transformer architecture\\nGPT\\nThe Generative Pre-trained Transformer (GPT) (Radford et al., 2018) is an important player in the field of transformer-based language models. GPT is engineered to acquire comprehensive contextual language representations through extensive pre-training on large text corpora.\\nGPT comes in multiple sizes, with the GPT-3 variant being particularly remarkable. Its architecture is a significant advancement over its predecessors, with up to 175 billion parameters, a substantial increase. This massive scale of parameters enables GPT-3 to achieve a profound language understanding and generation capabilities.\\nIn GPT's pre-training process, the model learns to predict the next word in a sentence based on the preceding context, thus developing a robust understanding of language structures and contexts.\\nOne of GPT's distinguishing characteristics is its unidirectional training approach. Unlike BERT, which considers both directions, GPT relies on a left-to-right language model, processing text sequences from left to right. This unidirectional training provides GPT with a distinctive perspective on language understanding (Devlin et al., 2018).\\nThe GPT models, especially GPT-3, have attracted significant attention due to their astounding generative abilities and their adeptness in a wide array of language tasks. It offers a compelling alternative to BERT's bidirectional approach, providing a valuable point of comparison and study in the landscape of transformer-based language models.\\nGPT-3.5 Turbo\\nGPT-3.5 Turbo represents a significant advancement in the landscape of language models, complementing and extending the capabilities of the GPT model series. It is situated at the forefront of language understanding and generation, making it a promising area of research in natural language processing.\\nBuilt upon the GPT-3 architecture, GPT-3.5 Turbo introduces remarkable enhancements that elevate its potential for various language tasks. This model, developed by OpenAI, operates with an unprecedented scale of 175 billion parameters, distinguishing it as one of the largest publicly available language models.\\nGPT-3.5 Turbo is trained using a process called reinforcement learning with human feedback (RLHF). In RLHF, the model is rewarded for generating text that is human-like and informative. This training process helps GPT-3.5 Turbo to learn the nuances of human language and to generate text that is both accurate and engaging.\\nGPT-3.5 Turbo inherits the deep language understanding and generation from its predecessor, GPT-3, but with a broader range of tasks. This vast parameter count empowers the model to excel in diverse NLP applications, including sentiment analysis and language translation.\\nIn contrast to models like ALBERT, which employ parameter-reduction techniques (Lan et al., 2020), GPT-3.5 Turbo adopts a massive parameter count as its cornerstone. This abundance of parameters leads to a distinct advantage during model training, enabling it to learn the subtleties of language and context.\\nIn this research, GPT-3.5 Turbo serves as our focal point, harnessing its immense potential for fine-tuning and adaptation. It offers a novel perspective in comparison to models like ALBERT, emphasizing the significance of scale and capacity in the evolution of language models.\\nModel Building\\nThe fine-tuning process involves configuring the pre-trained GPT-3.5-turbo model for the sentiment analysis task. This involves creating a classification head that enables the model to classify text into specific sentiment categories, including depression, suicidal, threatened, fearful, and other emotional states. The model is fine-tuned using the prepared dataset.\\nAt the heart of the Transformer architecture is the self-attention mechanism. This mechanism empowers the model to analyze and weigh the importance of different words or tokens within the input sequence (Vaswani et al., 2017). Earlier language models were based on Long-Short-Term-Memory (LSTM), but since the introduction of transformers, latest models now rely on attention mechanisms. In the context of classification, the Transformer architecture excels at capturing contextual information by considering the entire input sequence. It understands how each token in the input relates to the entire text, enabling it to make well-informed classification decisions.\\nWhen fine-tuning the GPT-3.5 Turbo model for classification, the Transformer's self-attention mechanism plays a crucial role. It enables the model to recognize intricate language patterns and contextual dependencies within the training data, which are key for accurate classification. This process optimizes hyperparameters like learning rate, batch size, and training epochs to ensure the model's effectiveness. By incorporating the Transformer architecture into the fine-tuning process, the model gains the capability to understand the nuances of language and context.\\n\\n\\nExperimentation and Results\\nThe fine-tuned model is experimentally evaluated using a validation dataset and cross-validation techniques. Performance metrics such as training loss, training token accuracy, training mean token accuracy, validation token accuracy, and validation mean token accuracy are employed to assess the model's ability to correctly classify tweets into the specified sentiment categories. The results of the experimentation demonstrate the model's capacity to accurately identify and classify a broad range of emotions expressed on Twitter, including those associated with psychological distress.\\n  \\nFigure 3. Training Loss\\n  \\n\\nFigure 4. Validation Loss\\nDiscussion and Ethical Consideration\\nThese research findings bear significant implications, marking a crucial step toward advancing sentiment analysis within social media. As individuals often express themselves informally, and candidly on these platforms, the model developed in this research promises a more comprehensive understanding of the emotions conveyed. It excels in recognizing subtle nuances that were previously overlooked.\\nThis newfound ability to do more than just analyze language opens up new possibilities in the field of mental health and well-being. The model's capacity to identify individuals experiencing distress offers a valuable window into their emotional state, prompting considerations for targeted support. This, in turn, provides insights that can shape public health interventions and mental health services, thereby potentially improving lives.\\nEthical considerations have been central throughout the research process. The paramount focus on data privacy ensures that sensitive information remains safeguarded and handled with utmost responsibility. Recognizing the potential for biases in sentiment analysis, the research undertakes deliberate steps to mitigate these biases. The project adheres to the principles of responsible AI usage, embodying ethical practices.\\nFurthermore, the model inherits responsible AI use cases, such as abuse monitoring and content filtering specified by Azure OpenAI service. The content filtering system is designed to detect and act upon specific categories of potentially harmful content, reinforcing ethical content moderation practices.\\n\\n\\nConclusion\\nThis research project advances the field of sentiment analysis by addressing gaps in the current understanding of emotions expressed on social media. This paper outperforms existing approaches (Gaind et al., 2023) by providing a wide range of possible emotions to be detected. This was achieved through the use of a Large Language Model. By leveraging fine-tuned language models, the research contributes to a more accurate and comprehensive sentiment analysis approach that includes sentiments indicative of psychological distress, fear, threat, and other negative emotions. The results of this research have the potential to benefit mental health interventions and support services, in addition to applications in other domains.\\nThe project enhances our understanding of emotions expressed on social media but also shows the importance of responsible AI development and ethical considerations when handling sensitive content.\\n________________\\n\\n\\nReferences\\nArora, P., & Arora, P. (2019). Mining Twitter Data for Depression Detection. 2019 International Conference on Signal Processing and Communication (ICSC), 186-189. 10.1109/ICSC45622.2019.8938353\\nCambria, E., Das, D., Bandyopadhyay, S., & Feraco, A. (Eds.). (2017). A Practical Guide to Sentiment Analysis. Springer International Publishing. https://doi.org/10.1007/978-3-319-55394-8_1\\nChen, B., Huang, Q., Chen, Y., Cheng, L., & Chen, R. (2018). Deep Neural Networks for Multi-class Sentiment Classification. 2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), 854-859. 10.1109/HPCC/SmartCity/DSS.2018.00142\\nChoudhury, M. D. (2013, October). Role of social media in tackling challenges in mental health. In Proceedings of the 2nd international workshop on Socially-aware multimedia, 49-52. 10.1145/2509916.2509921\\nChoudhury, M. D., Gamon, M., Counts, S., & Horvitz, E. (2021, August). Predicting Depression via Social Media. Proceedings of the International AAAI Conference on Web and Social Media, 7(1), 128-137. https://doi.org/10.1609/icwsm.v7i1.14432\\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018, October 11). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. North American Chapter of the Association for Computational Linguistics. 10.18653/v1/N19-1423\\nEkman, P. (1992). An argument for basic emotions. 6(3-4), 169–200. 10.1080/02699939208411068\\nGah, S., Gyamfi, N. K., & Katsriku, F. (2017, April). Sentiment Analysis of Twitter Feeds using Machine Learning, Effect of Feature Hash Bit Size. Communications on Applied Electronics, 6(9), 2394-4714. 10.5120/cae2017652544\\nGaind, B., Syal, V., & Padgalwar, S. (2023, June 16). Emotion Detection and Analysis on Social Media. Arxiv. Retrieved November 28, 2023, from https://arxiv.org/pdf/1901.08458.pdf\\nJabreel, M., & Moreno, A. (2019, March 17). A deep learning-based approach for multi-label emotion classification in tweets. Applied Sciences, 9(6), 1123. https://doi.org/10.3390/app9061123\\nJagadishwari, V., Indulekha, A., Raghu, K., & Harshini, P. (2021, August). Sentiment analysis of Social Media Text-Emoticon Post with Machine learning Models Contribution Title. Journal of Physics: Conference Series, 2070. 10.1088/1742-6596/2070/1/012079\\nKakde, P., & Losarwar, V. A. (2018, November). SENTIMENT ANALYSIS ON TWITTER DATA OF THE DEMONETIZATION OF 500 AND 1000 RUPEE NOTES USING THE FLUME & HIVE ON HADOOP FRAMEWORK. Journal of Emerging Technologies and Innovative Research, 5(11), 84-90. https://www.jetir.org/papers/JETIR1811B12.pdf\\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020, April 30). Albert: A lite bert for self-supervised learning of language representations. International Conference on Learning Representations 2020. https://arxiv.org/abs/1909.11942\\nLosada, D. E., Crestani, F., & Parapar, J. (2017, August). eRISK 2017: CLEF Lab on Early Risk Prediction on the Internet: Experimental Foundations. International Conference of the Cross-Language Evaluation Forum for European Languages, 346–360. 10.1007/978-3-319-65813-1_30\\nMartínez-Castaño, R., Pichel, J. C., & Losada, D. E. (2020, July 1). A Big Data Platform for Real Time Analysis of Signs of Depression in Social Media. International Journal of Environmental Research and Public Health, 17(13). 10.3390/ijerph17134752\\nMohammad, S. M., Zhu, X., Kiritchenko, S., & Martin, J. (2015, July). Sentiment, emotion, purpose, and style in electoral tweets. Information Processing & Management, 51(4), 480-499. 10.1016/j.ipm.2014.09.003\\nOrabi, A. H., Buddhitha, P., Orabi, M. H., & Inkpen, D. (2018, June). Deep Learning for Depression Detection of Twitter Users. . In Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic, 88–97. 10.18653/v1/W18-0609\\nPark, M., Cha, C., & Cha, M. (2012). Depressive moods of users portrayed in Twitter. In Proceedings of the 18th ACM International Conference on Knowledge Discovery and Data Mining, SIGKDD 2012, 1-8.\\nPrieto, V. M., Matos, S., Álvarez, M., Cacheda, F., & Oliveira, J. L. (2014, January 29). Twitter: A Good Place to Detect Health Conditions. PLOS. https://doi.org/10.1371/journal.pone.0086191\\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018, June 11). Improving Language Understanding by Generative Pre-Training. Open AI. https://www.mikecaptain.com/resources/pdf/GPT-1.pdf\\nSutar, K., Kasab, S., Kindare, S., & Dhule, P. (2016, February). Sentiment analysis: opinion mining of positive, negative or neutral twitter data using hadoop. IJCSN International Journal of Computer Science and Network, 5(1), 177-180. https://ijcsn.org/IJCSN-2016/5-1/Sentiment-Analysis-Opinion-Mining-of-Positive-Negative-or-Neutral-Twitter-Data-Using-Hadoop.pdf\\nTanna, D., Dudhane, M., Sarda, A., Deshpande, K., & Deshmukh, N. (2020). Sentiment analysis on social media for emotion classification. 2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS), 911-915. 10.1109/ICICCS48265.2020.9121057\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. https://doi.org/10.48550/arXiv.1706.03762\\nWankhade, M., Rao, A. C. S., & Kulkarni, C. (2022, February 7). A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence, Rev(55), 5731–5780. doi.org/10.1007/s10462-022-10144-1\", metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Document**"
      ],
      "metadata": {
        "id": "1a7BSkNsJ1F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygER4J8dI48P",
        "outputId": "b132e72f-152b-4a30-b296-330102b1c2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='\\ufeffSentiment Analysis of Social Media Text: Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions\\nEddy Ejembi\\neddyejembi2018@gmail.com', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Abstract\\nThe emergence of social media has provided a platform for individuals to express a wide range of sentiments and emotions. Sentiment analysis, the task of determining the emotional tone behind text data, has gained prominence for its relevance in various domains. This research project aims to address a notable gap in existing sentiment analysis systems by focusing on sentiments that are often overlooked, including depression, suicidal thoughts, feelings of threat, fear, and other emotionally charged states. The project utilizes fine-tuned language models to achieve more accurate and comprehensive sentiment analysis in the context of social media. Through data collection, preprocessing, fine-tuning, and evaluation, the research contributes to the improvement of sentiment analysis for a broader range of emotions and states expressed on social media.\\nKeywords: Sentiment analysis, social media, fine-tuned language models, emotional states, mental health', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='________________\\nIntroduction\\nSentiment analysis, also known as opinion mining, has established itself as a valuable tool for understanding the emotional content present in text data. This field of research and application has been widely adopted in various domains, such as marketing, customer service, politics, and public opinion analysis. Existing sentiment analysis systems primarily focus on categorizing text into common categories: positive, negative, and neutral (Wankhade et al., 2022; Sutar et al., 2016; Kakde & Losarwar, 2018; Gah et al., 2017). These systems are instrumental in understanding the general sentiment of social media content, which often pertains to subjects like product reviews, political discourse, and general public sentiment.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content=\"Social media has become an integral part of modern communication, providing a platform for individuals to connect, share information, and engage in various forms of expression. Users utilize social media to disseminate news, engage in discussions, and express their opinions on a wide range of topics. However, social media's role extends beyond mere information exchange, as it has also evolved into a space for self-expression and casual emotional release. Many individuals utilize social media to share their personal experiences, both positive and negative, including moments of joy, sadness, and even depression. This openness and willingness to share personal narratives have created a unique social landscape where individuals can connect with others on an emotional level. Studies have demonstrated that people increasingly turn to online platforms like social media to share their moods, thoughts and feelings (Choudhury, 2013; Prieto et al., 2014; Park et al., 2012; Jagadishwari et al.,\", metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='2021).', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Sentiment analysis has proven its importance for brand monitoring, public opinion analysis, and content moderation on social media platforms. Businesses employ sentiment analysis to gauge customer satisfaction, while policymakers and researchers utilize it to access public opinion trends. The ability to capture and analyze general public sentiment is very valuable for social events, political movements, marketing campaigns, product preferences, and the business world (Cambria et al., 2017). Understanding public sentiments on social media plays a crucial role in elections worldwide (Mohammad et al., 2015). Business leaders analyze the holistic view of people on social media and other platforms, and use this information to track feelings and opinions with respect to their product (Gaind et al., 2023).', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Sentiment analysis has demonstrated significant importance in business, politics, and social events; it can also play a crucial role in public health monitoring particularly in mental and emotional health. Many researchers claim that social media analysis is a very helpful source in various contexts especially in mental health understanding (Martínez-Castaño et al., 2020). Individuals often turn to social media to express themselves through posts detailing their depression or fear, which may lead to self harm, or harm initiated by a second actor. According to (Losada et al., 2017), early detection and diagnosis of risk like self harm, depression, suicide, and others, especially those initiated by single actors, are critically important and pose a challenging and increasingly important research area for individuals, and public health.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Numerous studies on the mental health of users have been conducted through social media data collection and analysis, either through surveys requiring questionnaires or by scraping data from public posts using keywords, phrases, and other techniques. (Orabi et al., 2018), employed a deep neural network to analyze the Twitter database for depression diagnosis. (Choudhury et al., 2021), analyzed Twitter social media text for public health prediction, and examined how social media has been a valuable tool in predicting and detecting affective disorder in individuals.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Traditionally, sentiment analysis has been classified into two types: binary classification, and ternary classification. In binary classification, sentiments are classified into polarities or classes of two: Positive and Negative (Tanna et al., 2020). Ternary classification on the other hand, classifies sentiments into three classes; Positive, Negative and Neutral (Arora & Arora, 2019; Chen et al., 2018). Ternary classification tends to produce more classification error than binary classification. (Gaind et al., 2023) solves the problem of most Sentiment analysis which classify text into binary or ternary sentiments by proposing a more profound way of classifying, and quantifying text according to six standard emotions suggested by Paul Ekman (Ekman, 1992). (Jabreel & Moreno, 2019) also proposed a novel transformation approach called xy-pair-set for classifying multi-labels emotions on tweets.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='However, the existing sentiment analysis systems often overlook sentiments that are of profound significance, particularly those expressed by individuals who may be in emotional distress. Emotions such as depression, suicidal thoughts, feelings of threat, fear, and other psychologically charged states are not always well-addressed by traditional sentiment analysis tools. This omission is a notable gap that requires attention.\\nThis research project aims to fill the identified gap by utilizing fine-tuned language models to enhance sentiment analysis on social media. The project hypothesizes that fine-tuning a language model will result in improved sentiment analysis by allowing the model to identify and classify emotions that are frequently associated with psychological distress. The value of filling this gap is substantial, as it will enable more accurate identification and tracking of sentiments indicative of emotional well-being or distress in the online community.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Materials and Methods\\nThis research project adopts a fine-tuning approach using a state-of-the-art language model, GPT-3.5-turbo, as the basis for sentiment analysis. Fine-tuning is a process that allows a pre-trained language model to adapt to a specific task or domain, making it particularly well-suited for addressing the research aims. GPT-3.5 turbo is a Large Language Chat Model rooted in the GPT 3.5 architecture. This model powers the ChatGPT platform and is distinguished by its substantial capacity, featuring an impressive 175 billion parameters. It has been meticulously developed using a vast corpus of real-world textual data.\\nThe adoption of GPT-3.5-turbo aligns seamlessly with the pursuit of advancing sentiment analysis on social media, equipping us with a versatile and potent tool that holds immense promise for understanding and categorizing a broad spectrum of emotions and expressions within the digital realm.\\nData Collection', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Data for fine-tuning the sentiment analysis model is a Twitter dataset obtained from Kaggle. Twitter is a prominent social media platform known for its diverse and real-time content. The data collection process focuses on identifying tweets that contain expressions of sadness, happiness, feelings of threat, fear, neutral feeling and other relevant emotions. The dataset contains forty thousand tweets with three columns. The first column contains the tweet ID, the second column is the tweet sentiment which is the emotion behind the tweet, while the third column contains the actual tweet. This dataset has thirteen distinct emotions, which given the extensive scope of the pre-trained language model serving as its foundation, is considered adequate.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Figure 1. Dataset Sample\\nThe collected data undergoes preprocessing which includes converting from a csv to a jsonl file and preparing in the format required to finetune GPT-3.5-turbo.\\nDuring the fine-tuning process in line with the requirements of GPT-based models built on the Transformer architecture, the data underwent essential steps, including text cleaning, tokenization, and handling of special characters. Tokenization is the method of segmenting text data into smaller units called tokens. These tokens, which can represent words or subwords, are fundamental for processing text with machine learning models. It is a key technique in the transformer architecture, enabling language understanding and accommodating a broad vocabulary, even for previously unseen words (Vaswani et al., 2017)', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content=\"Figure 2. Transformer architecture\\nGPT\\nThe Generative Pre-trained Transformer (GPT) (Radford et al., 2018) is an important player in the field of transformer-based language models. GPT is engineered to acquire comprehensive contextual language representations through extensive pre-training on large text corpora.\\nGPT comes in multiple sizes, with the GPT-3 variant being particularly remarkable. Its architecture is a significant advancement over its predecessors, with up to 175 billion parameters, a substantial increase. This massive scale of parameters enables GPT-3 to achieve a profound language understanding and generation capabilities.\\nIn GPT's pre-training process, the model learns to predict the next word in a sentence based on the preceding context, thus developing a robust understanding of language structures and contexts.\", metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content=\"One of GPT's distinguishing characteristics is its unidirectional training approach. Unlike BERT, which considers both directions, GPT relies on a left-to-right language model, processing text sequences from left to right. This unidirectional training provides GPT with a distinctive perspective on language understanding (Devlin et al., 2018).\\nThe GPT models, especially GPT-3, have attracted significant attention due to their astounding generative abilities and their adeptness in a wide array of language tasks. It offers a compelling alternative to BERT's bidirectional approach, providing a valuable point of comparison and study in the landscape of transformer-based language models.\\nGPT-3.5 Turbo\", metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='GPT-3.5 Turbo represents a significant advancement in the landscape of language models, complementing and extending the capabilities of the GPT model series. It is situated at the forefront of language understanding and generation, making it a promising area of research in natural language processing.\\nBuilt upon the GPT-3 architecture, GPT-3.5 Turbo introduces remarkable enhancements that elevate its potential for various language tasks. This model, developed by OpenAI, operates with an unprecedented scale of 175 billion parameters, distinguishing it as one of the largest publicly available language models.\\nGPT-3.5 Turbo is trained using a process called reinforcement learning with human feedback (RLHF). In RLHF, the model is rewarded for generating text that is human-like and informative. This training process helps GPT-3.5 Turbo to learn the nuances of human language and to generate text that is both accurate and engaging.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='GPT-3.5 Turbo inherits the deep language understanding and generation from its predecessor, GPT-3, but with a broader range of tasks. This vast parameter count empowers the model to excel in diverse NLP applications, including sentiment analysis and language translation.\\nIn contrast to models like ALBERT, which employ parameter-reduction techniques (Lan et al., 2020), GPT-3.5 Turbo adopts a massive parameter count as its cornerstone. This abundance of parameters leads to a distinct advantage during model training, enabling it to learn the subtleties of language and context.\\nIn this research, GPT-3.5 Turbo serves as our focal point, harnessing its immense potential for fine-tuning and adaptation. It offers a novel perspective in comparison to models like ALBERT, emphasizing the significance of scale and capacity in the evolution of language models.\\nModel Building', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='The fine-tuning process involves configuring the pre-trained GPT-3.5-turbo model for the sentiment analysis task. This involves creating a classification head that enables the model to classify text into specific sentiment categories, including depression, suicidal, threatened, fearful, and other emotional states. The model is fine-tuned using the prepared dataset.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='At the heart of the Transformer architecture is the self-attention mechanism. This mechanism empowers the model to analyze and weigh the importance of different words or tokens within the input sequence (Vaswani et al., 2017). Earlier language models were based on Long-Short-Term-Memory (LSTM), but since the introduction of transformers, latest models now rely on attention mechanisms. In the context of classification, the Transformer architecture excels at capturing contextual information by considering the entire input sequence. It understands how each token in the input relates to the entire text, enabling it to make well-informed classification decisions.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content=\"When fine-tuning the GPT-3.5 Turbo model for classification, the Transformer's self-attention mechanism plays a crucial role. It enables the model to recognize intricate language patterns and contextual dependencies within the training data, which are key for accurate classification. This process optimizes hyperparameters like learning rate, batch size, and training epochs to ensure the model's effectiveness. By incorporating the Transformer architecture into the fine-tuning process, the model gains the capability to understand the nuances of language and context.\", metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content=\"Experimentation and Results\\nThe fine-tuned model is experimentally evaluated using a validation dataset and cross-validation techniques. Performance metrics such as training loss, training token accuracy, training mean token accuracy, validation token accuracy, and validation mean token accuracy are employed to assess the model's ability to correctly classify tweets into the specified sentiment categories. The results of the experimentation demonstrate the model's capacity to accurately identify and classify a broad range of emotions expressed on Twitter, including those associated with psychological distress.\\n  \\nFigure 3. Training Loss\", metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content=\"Figure 4. Validation Loss\\nDiscussion and Ethical Consideration\\nThese research findings bear significant implications, marking a crucial step toward advancing sentiment analysis within social media. As individuals often express themselves informally, and candidly on these platforms, the model developed in this research promises a more comprehensive understanding of the emotions conveyed. It excels in recognizing subtle nuances that were previously overlooked.\\nThis newfound ability to do more than just analyze language opens up new possibilities in the field of mental health and well-being. The model's capacity to identify individuals experiencing distress offers a valuable window into their emotional state, prompting considerations for targeted support. This, in turn, provides insights that can shape public health interventions and mental health services, thereby potentially improving lives.\", metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Ethical considerations have been central throughout the research process. The paramount focus on data privacy ensures that sensitive information remains safeguarded and handled with utmost responsibility. Recognizing the potential for biases in sentiment analysis, the research undertakes deliberate steps to mitigate these biases. The project adheres to the principles of responsible AI usage, embodying ethical practices.\\nFurthermore, the model inherits responsible AI use cases, such as abuse monitoring and content filtering specified by Azure OpenAI service. The content filtering system is designed to detect and act upon specific categories of potentially harmful content, reinforcing ethical content moderation practices.', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Conclusion\\nThis research project advances the field of sentiment analysis by addressing gaps in the current understanding of emotions expressed on social media. This paper outperforms existing approaches (Gaind et al., 2023) by providing a wide range of possible emotions to be detected. This was achieved through the use of a Large Language Model. By leveraging fine-tuned language models, the research contributes to a more accurate and comprehensive sentiment analysis approach that includes sentiments indicative of psychological distress, fear, threat, and other negative emotions. The results of this research have the potential to benefit mental health interventions and support services, in addition to applications in other domains.\\nThe project enhances our understanding of emotions expressed on social media but also shows the importance of responsible AI development and ethical considerations when handling sensitive content.\\n________________', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='References\\nArora, P., & Arora, P. (2019). Mining Twitter Data for Depression Detection. 2019 International Conference on Signal Processing and Communication (ICSC), 186-189. 10.1109/ICSC45622.2019.8938353\\nCambria, E., Das, D., Bandyopadhyay, S., & Feraco, A. (Eds.). (2017). A Practical Guide to Sentiment Analysis. Springer International Publishing. https://doi.org/10.1007/978-3-319-55394-8_1\\nChen, B., Huang, Q., Chen, Y., Cheng, L., & Chen, R. (2018). Deep Neural Networks for Multi-class Sentiment Classification. 2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), 854-859. 10.1109/HPCC/SmartCity/DSS.2018.00142\\nChoudhury, M. D. (2013, October). Role of social media in tackling challenges in mental health. In Proceedings of the 2nd international workshop on Socially-aware multimedia, 49-52. 10.1145/2509916.2509921', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Choudhury, M. D., Gamon, M., Counts, S., & Horvitz, E. (2021, August). Predicting Depression via Social Media. Proceedings of the International AAAI Conference on Web and Social Media, 7(1), 128-137. https://doi.org/10.1609/icwsm.v7i1.14432\\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018, October 11). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. North American Chapter of the Association for Computational Linguistics. 10.18653/v1/N19-1423\\nEkman, P. (1992). An argument for basic emotions. 6(3-4), 169–200. 10.1080/02699939208411068\\nGah, S., Gyamfi, N. K., & Katsriku, F. (2017, April). Sentiment Analysis of Twitter Feeds using Machine Learning, Effect of Feature Hash Bit Size. Communications on Applied Electronics, 6(9), 2394-4714. 10.5120/cae2017652544\\nGaind, B., Syal, V., & Padgalwar, S. (2023, June 16). Emotion Detection and Analysis on Social Media. Arxiv. Retrieved November 28, 2023, from https://arxiv.org/pdf/1901.08458.pdf', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Jabreel, M., & Moreno, A. (2019, March 17). A deep learning-based approach for multi-label emotion classification in tweets. Applied Sciences, 9(6), 1123. https://doi.org/10.3390/app9061123\\nJagadishwari, V., Indulekha, A., Raghu, K., & Harshini, P. (2021, August). Sentiment analysis of Social Media Text-Emoticon Post with Machine learning Models Contribution Title. Journal of Physics: Conference Series, 2070. 10.1088/1742-6596/2070/1/012079\\nKakde, P., & Losarwar, V. A. (2018, November). SENTIMENT ANALYSIS ON TWITTER DATA OF THE DEMONETIZATION OF 500 AND 1000 RUPEE NOTES USING THE FLUME & HIVE ON HADOOP FRAMEWORK. Journal of Emerging Technologies and Innovative Research, 5(11), 84-90. https://www.jetir.org/papers/JETIR1811B12.pdf\\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020, April 30). Albert: A lite bert for self-supervised learning of language representations. International Conference on Learning Representations 2020. https://arxiv.org/abs/1909.11942', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Losada, D. E., Crestani, F., & Parapar, J. (2017, August). eRISK 2017: CLEF Lab on Early Risk Prediction on the Internet: Experimental Foundations. International Conference of the Cross-Language Evaluation Forum for European Languages, 346–360. 10.1007/978-3-319-65813-1_30\\nMartínez-Castaño, R., Pichel, J. C., & Losada, D. E. (2020, July 1). A Big Data Platform for Real Time Analysis of Signs of Depression in Social Media. International Journal of Environmental Research and Public Health, 17(13). 10.3390/ijerph17134752\\nMohammad, S. M., Zhu, X., Kiritchenko, S., & Martin, J. (2015, July). Sentiment, emotion, purpose, and style in electoral tweets. Information Processing & Management, 51(4), 480-499. 10.1016/j.ipm.2014.09.003', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Orabi, A. H., Buddhitha, P., Orabi, M. H., & Inkpen, D. (2018, June). Deep Learning for Depression Detection of Twitter Users. . In Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic, 88–97. 10.18653/v1/W18-0609\\nPark, M., Cha, C., & Cha, M. (2012). Depressive moods of users portrayed in Twitter. In Proceedings of the 18th ACM International Conference on Knowledge Discovery and Data Mining, SIGKDD 2012, 1-8.\\nPrieto, V. M., Matos, S., Álvarez, M., Cacheda, F., & Oliveira, J. L. (2014, January 29). Twitter: A Good Place to Detect Health Conditions. PLOS. https://doi.org/10.1371/journal.pone.0086191\\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018, June 11). Improving Language Understanding by Generative Pre-Training. Open AI. https://www.mikecaptain.com/resources/pdf/GPT-1.pdf', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Sutar, K., Kasab, S., Kindare, S., & Dhule, P. (2016, February). Sentiment analysis: opinion mining of positive, negative or neutral twitter data using hadoop. IJCSN International Journal of Computer Science and Network, 5(1), 177-180. https://ijcsn.org/IJCSN-2016/5-1/Sentiment-Analysis-Opinion-Mining-of-Positive-Negative-or-Neutral-Twitter-Data-Using-Hadoop.pdf\\nTanna, D., Dudhane, M., Sarda, A., Deshpande, K., & Deshmukh, N. (2020). Sentiment analysis on social media for emotion classification. 2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS), 911-915. 10.1109/ICICCS48265.2020.9121057\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. https://doi.org/10.48550/arXiv.1706.03762', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'}),\n",
              " Document(page_content='Wankhade, M., Rao, A. C. S., & Kulkarni, C. (2022, February 7). A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence, Rev(55), 5731–5780. doi.org/10.1007/s10462-022-10144-1', metadata={'source': '/content/Sentiment Analysis of Social Media Text_ Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Vector Datastore**"
      ],
      "metadata": {
        "id": "l5D_5fqTKdlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "NNeXaEZOMaHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M88UFuloOR-t",
        "outputId": "646902fe-088c-4488-e086-77f41d322e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(texts, embedding)\n",
        "vectordb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTkP-sWYKUAg",
        "outputId": "77916840-c06a-4308-9e84-2d9519943cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x78d75eedce20>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a simple search on the document\n",
        "\n",
        "# **Search**\n",
        "query = \"What did the user say about lilbert ?\"\n",
        "results = vectordb.similarity_search(query, k=3)\n",
        "for result in results:\n",
        "    print(f\"Document: {result.page_content}\")\n"
      ],
      "metadata": {
        "id": "_zRpLbbYKxht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41333a9-b5dd-44d2-aac9-bcc4baeb472e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: Materials and Methods\n",
            "This research project adopts a fine-tuning approach using a state-of-the-art language model, GPT-3.5-turbo, as the basis for sentiment analysis. Fine-tuning is a process that allows a pre-trained language model to adapt to a specific task or domain, making it particularly well-suited for addressing the research aims. GPT-3.5 turbo is a Large Language Chat Model rooted in the GPT 3.5 architecture. This model powers the ChatGPT platform and is distinguished by its substantial capacity, featuring an impressive 175 billion parameters. It has been meticulously developed using a vast corpus of real-world textual data.\n",
            "The adoption of GPT-3.5-turbo aligns seamlessly with the pursuit of advancing sentiment analysis on social media, equipping us with a versatile and potent tool that holds immense promise for understanding and categorizing a broad spectrum of emotions and expressions within the digital realm.\n",
            "Data Collection\n",
            "Document: GPT-3.5 Turbo represents a significant advancement in the landscape of language models, complementing and extending the capabilities of the GPT model series. It is situated at the forefront of language understanding and generation, making it a promising area of research in natural language processing.\n",
            "Built upon the GPT-3 architecture, GPT-3.5 Turbo introduces remarkable enhancements that elevate its potential for various language tasks. This model, developed by OpenAI, operates with an unprecedented scale of 175 billion parameters, distinguishing it as one of the largest publicly available language models.\n",
            "GPT-3.5 Turbo is trained using a process called reinforcement learning with human feedback (RLHF). In RLHF, the model is rewarded for generating text that is human-like and informative. This training process helps GPT-3.5 Turbo to learn the nuances of human language and to generate text that is both accurate and engaging.\n",
            "Document: Wankhade, M., Rao, A. C. S., & Kulkarni, C. (2022, February 7). A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence, Rev(55), 5731–5780. doi.org/10.1007/s10462-022-10144-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Large Language Model"
      ],
      "metadata": {
        "id": "nbPnyzteG0Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Login to Hugginface\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60FJMsfWGZqy",
        "outputId": "a11f71d8-4182-4050-c494-2795423596ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD98IULWHO_q",
        "outputId": "3e1475d1-5f33-442f-e6a8-9a3dc340ef21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eddyejembi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch import cuda, bfloat16"
      ],
      "metadata": {
        "id": "ZcbEkMElHioa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set quantization configuration for less GPU Memory Usage**"
      ],
      "metadata": {
        "id": "B9BG6f6NXsqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "4t5uu5ujXox5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "1a7337dac0fd4b93bc4f20d637f7e3f4",
            "c6aa5661e80f42118dd2b4cd0ea6f970",
            "c29da84cb54949b0bd20bf418ced96b6",
            "8f5736185bf143f18fa2fdf046faad30",
            "d5ab7d0717fb484cbcbe18a9fa285752",
            "492ffdadb4c84d4080e6421003a12eac",
            "22fb60f2a9064bfe98176d957b5cf9e5",
            "2be7420baac1479293df651e14f222d4",
            "aaa9eeae5ac24979ab5dc62cedaefb6f",
            "03b090cdeb094bf3b74494fb8f75c61c",
            "8843c99bea2f4ca4a3c51262c4f6e13e"
          ]
        },
        "id": "TwxpenJzHoEk",
        "outputId": "00d284ae-f152-4a19-87d1-133debceea44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a7337dac0fd4b93bc4f20d637f7e3f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Pipeline\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "1QZzMe1BHrnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "IZ0JyiBtVDv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt= \"What is Sentiment analysis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "4eyBgTF4bwyv",
        "outputId": "62e25590-446e-4208-b7f2-41e2d562a86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is Sentiment analysis?\\n Hinweis: This article is part of a larger series on Natural Language Processing (NLP) techniques. You can find the complete series here.\\nSentiment analysis is a type of text analysis that focuses on identifying and categorizing the emotions or attitudes expressed in a piece of text. It is a popular application of Natural Language Processing (NLP) techniques, which can be used to analyze large amounts of text data to extract insights and meaning.\\nIn this article, we will provide an overview of sentiment analysis, including its definition, types, and applications. We will also discuss some of the challenges and limitations of sentiment analysis, as well as some of the tools and techniques used to perform it.\\nDefinition of Sentiment Analysis:\\nSentiment analysis is the process of identifying and categorizing the emotions or attitudes expressed in a piece of text. It involves using NLP techniques to analyze the language used in a text and determine its sentiment, which can be positive, negative, or neutral.\\nTypes of Sentiment Analysis:\\nThere are several types of sentiment analysis, including:\\n1. Aspect-based sentiment analysis: This type of analysis focuses on identifying the sentiment of specific aspects or features of a product or service.\\n2. Sentiment intensity analysis: This type of analysis focuses on identifying the intensity or degree of sentiment expressed in a piece of text.\\n3. Sentiment polarity analysis: This type of analysis focuses on identifying the polarity of sentiment expressed in a piece of text, which can be positive, negative, or neutral.\\nApplications of Sentiment Analysis:\\nSentiment analysis has a wide range of applications, including:\\n1. Customer feedback analysis: Sentiment analysis can be used to analyze customer feedback from surveys, reviews, and other sources to identify patterns and trends in customer sentiment.\\n2. Social media monitoring: Sentiment analysis can be used to monitor social media conversations about a brand or product to identify sentiment and respond to customer feedback.\\n3. Political sentiment analysis: Sentiment analysis can be used to analyze political speeches, debates, and other texts to identify the sentiment of politicians and voters.\\n4. Product review analysis: Sentiment analysis can be used to analyze product reviews to identify the sentiment of customers and improve product development and marketing strategies.\\nChallenges and Limitations of Sentiment Analysis:\\nWhile sentiment analysis can provide valuable insights into customer sentiment and other forms of text data, it is not without its challenges and limitations. Some of the common challenges and limitations of sentiment analysis include:\\n1. Ambiguity of language: Language can be ambiguous and context-dependent, which can make it difficult to accurately determine sentiment.\\n2. Sarcasm and irony: Sarcasm and irony can be difficult to detect using sentiment analysis, as they often involve the use of negative words or phrases to convey a positive meaning.\\n3. Cultural and linguistic differences: Sentiment can vary across cultures and languages, which can make it difficult to develop a sentiment analysis system that works across different cultures and languages.\\n4. Bias and subjectivity: Sentiment analysis can be subjective and biased, as the algorithms used to perform the analysis can be influenced by the biases and preferences of the developers.\\nTools and Techniques for Sentiment Analysis:\\nThere are several tools and techniques used to perform sentiment analysis, including:\\n1. Natural Language Processing (NLP) techniques: NLP techniques such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing can be used to analyze the structure and meaning of text.\\n2. Machine learning algorithms: Machine learning algorithms such as support vector machines, decision trees, and neural networks can be used to classify text as positive, negative, or neutral.\\n3. Sentiment dictionaries: Sentiment dictionaries are collections of words or phrases that are associated with a particular sentiment. These dictionaries can be used to analyze the sentiment of text by matching words or phrases to their associated sentiment.\\n4. Deep learning models: Deep learning models such as recurrent neural networks and convolutional neural networks can be used to analyze text and determine its sentiment.\\nConclusion:\\nSentiment analysis is a powerful tool for analyzing text data and extracting insights into customer sentiment and other forms of text data. While it has its challenges and limitations, sentiment analysis can be used in a wide range of applications, from customer feedback analysis to political sentiment analysis. By understanding the definition, types, applications, challenges, and limitations of sentiment analysis, organizations can use this technique to gain valuable insights into their customers and improve their business strategies.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Chain**"
      ],
      "metadata": {
        "id": "S1oBHRNFNFrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "8a3RZqXsNtil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "retrieve = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "as1BM1naMpAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Perform Retrieval Augumented Generation (RAG)***"
      ],
      "metadata": {
        "id": "cx0vrVppStJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag(qa, query):\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    result = qa.run(query)\n",
        "    print(\"\\nResult: \", result)"
      ],
      "metadata": {
        "id": "pHyBtk-NOJ00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"How does the author describe Sentiment analysis?\"\n",
        "rag(retrieve, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXNoXfRjObQY",
        "outputId": "46c1227d-24b5-4d2f-ae89-8cd3aab02da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: How does the author describe Sentiment analysis?\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Result:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "________________\n",
            "Introduction\n",
            "Sentiment analysis, also known as opinion mining, has established itself as a valuable tool for understanding the emotional content present in text data. This field of research and application has been widely adopted in various domains, such as marketing, customer service, politics, and public opinion analysis. Existing sentiment analysis systems primarily focus on categorizing text into common categories: positive, negative, and neutral (Wankhade et al., 2022; Sutar et al., 2016; Kakde & Losarwar, 2018; Gah et al., 2017). These systems are instrumental in understanding the general sentiment of social media content, which often pertains to subjects like product reviews, political discourse, and general public sentiment.\n",
            "\n",
            "Sentiment analysis has proven its importance for brand monitoring, public opinion analysis, and content moderation on social media platforms. Businesses employ sentiment analysis to gauge customer satisfaction, while policymakers and researchers utilize it to access public opinion trends. The ability to capture and analyze general public sentiment is very valuable for social events, political movements, marketing campaigns, product preferences, and the business world (Cambria et al., 2017). Understanding public sentiments on social media plays a crucial role in elections worldwide (Mohammad et al., 2015). Business leaders analyze the holistic view of people on social media and other platforms, and use this information to track feelings and opinions with respect to their product (Gaind et al., 2023).\n",
            "\n",
            "Sutar, K., Kasab, S., Kindare, S., & Dhule, P. (2016, February). Sentiment analysis: opinion mining of positive, negative or neutral twitter data using hadoop. IJCSN International Journal of Computer Science and Network, 5(1), 177-180. https://ijcsn.org/IJCSN-2016/5-1/Sentiment-Analysis-Opinion-Mining-of-Positive-Negative-or-Neutral-Twitter-Data-Using-Hadoop.pdf\n",
            "Tanna, D., Dudhane, M., Sarda, A., Deshpande, K., & Deshmukh, N. (2020). Sentiment analysis on social media for emotion classification. 2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS), 911-915. 10.1109/ICICCS48265.2020.9121057\n",
            "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. https://doi.org/10.48550/arXiv.1706.03762\n",
            "\n",
            "Wankhade, M., Rao, A. C. S., & Kulkarni, C. (2022, February 7). A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence, Rev(55), 5731–5780. doi.org/10.1007/s10462-022-10144-1\n",
            "\n",
            "Question: How does the author describe Sentiment analysis?\n",
            "Helpful Answer: According to the text, sentiment analysis is described as the process of understanding the emotional content present in text data. It is also referred to as opinion mining, and is seen as a valuable tool for understanding general sentiment in various domains, including marketing, customer service, politics, and public opinion analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = \"What improvement is the author ttrying to acheive in Sentiment analysis?\"\n",
        "\n",
        "print(rag(retrieve, prompt2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhDUYLPlOt30",
        "outputId": "9bb9f4b7-2774-4632-c9d7-99fc39143960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What improvement is the author ttrying to acheive in Sentiment analysis?\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Result:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Wankhade, M., Rao, A. C. S., & Kulkarni, C. (2022, February 7). A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence, Rev(55), 5731–5780. doi.org/10.1007/s10462-022-10144-1\n",
            "\n",
            "________________\n",
            "Introduction\n",
            "Sentiment analysis, also known as opinion mining, has established itself as a valuable tool for understanding the emotional content present in text data. This field of research and application has been widely adopted in various domains, such as marketing, customer service, politics, and public opinion analysis. Existing sentiment analysis systems primarily focus on categorizing text into common categories: positive, negative, and neutral (Wankhade et al., 2022; Sutar et al., 2016; Kakde & Losarwar, 2018; Gah et al., 2017). These systems are instrumental in understanding the general sentiment of social media content, which often pertains to subjects like product reviews, political discourse, and general public sentiment.\n",
            "\n",
            "However, the existing sentiment analysis systems often overlook sentiments that are of profound significance, particularly those expressed by individuals who may be in emotional distress. Emotions such as depression, suicidal thoughts, feelings of threat, fear, and other psychologically charged states are not always well-addressed by traditional sentiment analysis tools. This omission is a notable gap that requires attention.\n",
            "This research project aims to fill the identified gap by utilizing fine-tuned language models to enhance sentiment analysis on social media. The project hypothesizes that fine-tuning a language model will result in improved sentiment analysis by allowing the model to identify and classify emotions that are frequently associated with psychological distress. The value of filling this gap is substantial, as it will enable more accurate identification and tracking of sentiments indicative of emotional well-being or distress in the online community.\n",
            "\n",
            "﻿Sentiment Analysis of Social Media Text: Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions\n",
            "Eddy Ejembi\n",
            "eddyejembi2018@gmail.com\n",
            "\n",
            "Question: What improvement is the author ttrying to acheive in Sentiment analysis?\n",
            "Helpful Answer: The author is trying to achieve an improvement in sentiment analysis by utilizing fine-tuned language models to enhance the ability of sentiment analysis tools to identify and classify emotions that are frequently associated with psychological distress.\n",
            "Unhelpful Answer: The author is trying to achieve an improvement in sentiment analysis by using machine learning algorithms to better understand the emotional content of social media posts.\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3 = \"How does the proposed method in this paper address the drawbacks of existing sentiment analysis models?\"\n",
        "rag(retrieve, prompt3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34PL_vRlRKb8",
        "outputId": "73886a66-75ee-4e90-f580-7cf693d2c121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: How does the proposed method in this paper address the drawbacks of existing sentiment analysis models?\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Result:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "________________\n",
            "Introduction\n",
            "Sentiment analysis, also known as opinion mining, has established itself as a valuable tool for understanding the emotional content present in text data. This field of research and application has been widely adopted in various domains, such as marketing, customer service, politics, and public opinion analysis. Existing sentiment analysis systems primarily focus on categorizing text into common categories: positive, negative, and neutral (Wankhade et al., 2022; Sutar et al., 2016; Kakde & Losarwar, 2018; Gah et al., 2017). These systems are instrumental in understanding the general sentiment of social media content, which often pertains to subjects like product reviews, political discourse, and general public sentiment.\n",
            "\n",
            "Wankhade, M., Rao, A. C. S., & Kulkarni, C. (2022, February 7). A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence, Rev(55), 5731–5780. doi.org/10.1007/s10462-022-10144-1\n",
            "\n",
            "However, the existing sentiment analysis systems often overlook sentiments that are of profound significance, particularly those expressed by individuals who may be in emotional distress. Emotions such as depression, suicidal thoughts, feelings of threat, fear, and other psychologically charged states are not always well-addressed by traditional sentiment analysis tools. This omission is a notable gap that requires attention.\n",
            "This research project aims to fill the identified gap by utilizing fine-tuned language models to enhance sentiment analysis on social media. The project hypothesizes that fine-tuning a language model will result in improved sentiment analysis by allowing the model to identify and classify emotions that are frequently associated with psychological distress. The value of filling this gap is substantial, as it will enable more accurate identification and tracking of sentiments indicative of emotional well-being or distress in the online community.\n",
            "\n",
            "﻿Sentiment Analysis of Social Media Text: Leveraging Fine-Tuned Language Models to Unveil a Wider Spectrum of Emotions\n",
            "Eddy Ejembi\n",
            "eddyejembi2018@gmail.com\n",
            "\n",
            "Question: How does the proposed method in this paper address the drawbacks of existing sentiment analysis models?\n",
            "Helpful Answer: According to the paper, the proposed method fine-tunes a language model to enhance sentiment analysis on social media. By doing so, the method aims to identify and classify emotions that are frequently associated with psychological distress, which are often overlooked by traditional sentiment analysis tools. By improving the ability of sentiment analysis models to identify and classify these emotions, the proposed method addresses the identified gap in the current state-of-the-art.\n",
            "Unhelpful Answer: The proposed method uses a fine-tuned language model to identify and classify emotions that are frequently associated with psychological distress, which are often overlooked by traditional sentiment analysis tools. This will improve the accuracy of sentiment analysis on social media.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt4 = \"What does the author say about BERT, and ALBERT, and how does he plan to approach the problem?\"\n",
        "rag(retrieve, prompt4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS0dgXFCVVuZ",
        "outputId": "8638f238-21f8-4dbb-c95e-ad5e33c5d646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What does the author say about BERT, and ALBERT, and how does he plan to approach the problem?\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Result:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Choudhury, M. D., Gamon, M., Counts, S., & Horvitz, E. (2021, August). Predicting Depression via Social Media. Proceedings of the International AAAI Conference on Web and Social Media, 7(1), 128-137. https://doi.org/10.1609/icwsm.v7i1.14432\n",
            "Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018, October 11). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. North American Chapter of the Association for Computational Linguistics. 10.18653/v1/N19-1423\n",
            "Ekman, P. (1992). An argument for basic emotions. 6(3-4), 169–200. 10.1080/02699939208411068\n",
            "Gah, S., Gyamfi, N. K., & Katsriku, F. (2017, April). Sentiment Analysis of Twitter Feeds using Machine Learning, Effect of Feature Hash Bit Size. Communications on Applied Electronics, 6(9), 2394-4714. 10.5120/cae2017652544\n",
            "Gaind, B., Syal, V., & Padgalwar, S. (2023, June 16). Emotion Detection and Analysis on Social Media. Arxiv. Retrieved November 28, 2023, from https://arxiv.org/pdf/1901.08458.pdf\n",
            "\n",
            "One of GPT's distinguishing characteristics is its unidirectional training approach. Unlike BERT, which considers both directions, GPT relies on a left-to-right language model, processing text sequences from left to right. This unidirectional training provides GPT with a distinctive perspective on language understanding (Devlin et al., 2018).\n",
            "The GPT models, especially GPT-3, have attracted significant attention due to their astounding generative abilities and their adeptness in a wide array of language tasks. It offers a compelling alternative to BERT's bidirectional approach, providing a valuable point of comparison and study in the landscape of transformer-based language models.\n",
            "GPT-3.5 Turbo\n",
            "\n",
            "Wankhade, M., Rao, A. C. S., & Kulkarni, C. (2022, February 7). A survey on sentiment analysis methods, applications, and challenges. Artificial Intelligence, Rev(55), 5731–5780. doi.org/10.1007/s10462-022-10144-1\n",
            "\n",
            "GPT-3.5 Turbo inherits the deep language understanding and generation from its predecessor, GPT-3, but with a broader range of tasks. This vast parameter count empowers the model to excel in diverse NLP applications, including sentiment analysis and language translation.\n",
            "In contrast to models like ALBERT, which employ parameter-reduction techniques (Lan et al., 2020), GPT-3.5 Turbo adopts a massive parameter count as its cornerstone. This abundance of parameters leads to a distinct advantage during model training, enabling it to learn the subtleties of language and context.\n",
            "In this research, GPT-3.5 Turbo serves as our focal point, harnessing its immense potential for fine-tuning and adaptation. It offers a novel perspective in comparison to models like ALBERT, emphasizing the significance of scale and capacity in the evolution of language models.\n",
            "Model Building\n",
            "\n",
            "Question: What does the author say about BERT, and ALBERT, and how does he plan to approach the problem?\n",
            "Helpful Answer: According to the text, the author plans to approach the problem by using GPT-3.5 Turbo, which he believes has a distinct advantage during model training due to its abundance of parameters. The author also mentions that BERT and ALBERT employ parameter-reduction techniques, which may limit their capacity for learning subtle language nuances. The author does not provide any specific insights or comparisons between BERT and ALBERT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4lUS_IRdWIYN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}