{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waghmareps12/RANDOM_COLLAB_LLM_NOTEBOOKS/blob/main/Selecting_an_embedding_model_for_custom_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fb98851653cfa29b"
      },
      "cell_type": "markdown",
      "source": [
        "# Selecting an embedding model for your custom data"
      ],
      "id": "fb98851653cfa29b"
    },
    {
      "metadata": {
        "id": "a11c8e4a60143ecf"
      },
      "cell_type": "markdown",
      "source": [
        "We recommend reading the [\"Understanding embedding models: make an informed choice for your RAG\"](https://unstructured.io/blog/understanding-embedding-models-make-an-informed-choice-for-your-rag) blog post before proceeding with this tutorial.\n",
        "\n",
        "In this notebook, we'll build an end-to-end data processing pipeline using Unstructured Serverless API, and incorporate a model evaluation step into it. This way you can eliminate the guesswork - pick several promising candidates from the [Hugging Face MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), choose the best one for your specific data, and an embedding step with the best candidate to your Unstructured pipeline.\n",
        "\n",
        "We'll be comparing the performance of three embedding models from the MTEB leaderboard:\n",
        "* [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5): a strong general purpose embedding model that has 335M parameters, and achieves an average nDCG@10 of 54.29 on the MTEB leaderboard. On the financial-specific FiQA2018 dataset, it scores a respectable 45.02.\n",
        "* [mukaj/fin-mpnet-base](https://huggingface.co/mukaj/fin-mpnet-base): a compact text embedding model with only 109M parameters, fine-tuned on financial data. While its average nDCG@10 isn't listed on the leaderboard, it shines on FiQA2018 with an impressive score of 79.91.\n",
        "* [Snowflake/snowflake-arctic-embed-l](https://huggingface.co/Snowflake/snowflake-arctic-embed-l): a general purpose text embedding model with 334M parameters, ranks among the top performance on the leaderboard.  It achieved an average nDCG@10 of 55.98. On FiQA2018, its score is 44.71.\n",
        "\n",
        "To demonstrate the evaluation process, we'll use publicly available financial reports as \"custom data\", specifically, annual Form 10-K reports from a couple of Fortune 500 companies. These reports, required by the U.S. Securities and Exchange Commission (SEC), offer a deep dive into a company's financial performance. They go beyond the typical annual report, providing detailed information on corporate history, financial statements, earnings per share, and other crucial data points. For investors, 10-Ks are invaluable tools for making informed decisions. You can easily access and download these reports by visiting the website of any publicly traded US company.\n",
        "\n",
        "To reproduce the notebook, download the following PDFs, and place them into local `PDFS` directory:\n",
        "* Direct link: [Walmart Form-10K SEC filing for 2023](https://d18rn0p25nwr6d.cloudfront.net/CIK-0000104169/dfe6ee99-8fe6-4333-80ac-829d9e7595fa.pdf), or find it on [stock.walmart.com/financials](https://stock.walmart.com/financials/sec-filings/default.aspx)\n",
        "* Direct link: [Exxon Mobile Form-10K SEC filing for 2023](https://investor.exxonmobil.com/sec-filings/annual-reports/content/0000034088-24-000018/0000034088-24-000018.pdf), or find it on [https://ir.exxonmobil.com/sec-filings](https://ir.exxonmobil.com/sec-filings)\n",
        "\n",
        "To evaluate our chosen models on the Form-10-K PDFs as custom data, weâ€™ll go through the following steps:\n",
        "* Process the raw data from PDFs to ready-to-use chunks\n",
        "* Generate a synthetic evaluation dataset from the chunks\n",
        "* Set up and query three retrievers, each with a different embedding model\n",
        "* Compare performance and pick the best model\n",
        "* Integrate the best model into the pipeline as an embedding step\n"
      ],
      "id": "a11c8e4a60143ecf"
    },
    {
      "metadata": {
        "id": "4880c94377d80ec7"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-requisites and setup\n",
        "\n",
        "In this notebook we'll be generating a synthetic dataset, and you will need an LLM for that.\n",
        "We've run this notebook locally with `Llama3.1:8b` model via Ollama to generate a synthetic dataset. For a toy example with just two documents (even though they total to 386 pages!), this will be ok, but in a real-world scenario you may want to switch to a more powerful model, and potentially use a model provider, such as OpenAI, or Anthropic, for example.\n",
        "In this notebook, we've added an alternative code for calls to Claude3.5 Sonnet. Pick whichever you prefer.  \n",
        "\n",
        "To run the notebook locally with ollama:\n",
        "* Go to https://ollama.com and download the app for your OS, then pull the model onto your local machine: `ollama pull llama3.1:8b`\n",
        "* `pip install ollama`\n",
        "\n",
        "To run the notebook with Claude3.5 Sonnet from Anthropic:\n",
        "* Acquire an [Anthropic API key](https://www.anthropic.com/claude) and save it to a local `.env` file as `ANTHROPIC_API_KEY`.\n",
        "* `pip install anthropic`  \n",
        "\n",
        "Install the rest of the necessary:\n",
        "\n",
        "* `unstructured` & `unstructured-ingest` for preprocessing documents.\n",
        "* `python-dotenv` to load the environment variables from a `.env` file\n",
        "* `chromadb` and `langchain` to set up retrievers with different embedding models\n",
        "\n",
        "To use this example, you'll need to get an [Unstructured API key](https://unstructured.io/api-key-hosted). The Unstructured Serverless API comes with a 14-day trial capped at 1000 pages per day.\n",
        "Save the Unstructured API key as `UNSTRUCTURED_API_KEY`, and Unstructured API URL (you can get it from your personal dashboard) as `UNSTRUCTURED_URL` in a local `.env` file."
      ],
      "id": "4880c94377d80ec7"
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "source": [
        "!pip install -qU \"unstructured-ingest[pdf, embed-huggingface]\" unstructured python-dotenv langchain chromadb ollama"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "de47b15154d698cb"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the environment variables"
      ],
      "id": "de47b15154d698cb"
    },
    {
      "metadata": {
        "id": "956ca69e6e8aa491"
      },
      "cell_type": "markdown",
      "source": [
        "Load the environment variables from a local file."
      ],
      "id": "956ca69e6e8aa491"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T14:48:16.118136Z",
          "start_time": "2024-08-20T14:48:16.113086Z"
        },
        "id": "e8b7e875f10eceec",
        "outputId": "2a25b6cb-4d27-4591-e590-929d081bff70"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv('.env')"
      ],
      "id": "e8b7e875f10eceec",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "803404ac8f388566"
      },
      "cell_type": "markdown",
      "source": [
        "Import the libraries. Uncomment `import anthropic` if you plan to use Claude."
      ],
      "id": "803404ac8f388566"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T16:27:21.183973Z",
          "start_time": "2024-08-20T16:27:19.909054Z"
        },
        "id": "b5e20edbe453e280"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import ollama\n",
        "# import anthropic\n",
        "import pandas as pd\n",
        "\n",
        "from unstructured_ingest.v2.pipeline.pipeline import Pipeline\n",
        "from unstructured_ingest.v2.interfaces import ProcessorConfig\n",
        "from unstructured_ingest.v2.processes.connectors.local import (\n",
        "    LocalIndexerConfig,\n",
        "    LocalDownloaderConfig,\n",
        "    LocalConnectionConfig,\n",
        "    LocalUploaderConfig\n",
        ")\n",
        "from unstructured_ingest.v2.processes.partitioner import PartitionerConfig\n",
        "from unstructured_ingest.v2.processes.chunker import ChunkerConfig\n",
        "from unstructured_ingest.v2.processes.embedder import EmbedderConfig\n",
        "\n",
        "from unstructured.staging.base import elements_from_json\n",
        "from unstructured.staging.base import elements_to_dicts\n",
        "from unstructured.staging.base import dict_to_elements\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.vectorstores import utils as chromautils\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "id": "b5e20edbe453e280",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "356e8798d2b085ff"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess PDFs from a source location\n",
        "\n",
        "The Form 10-K reports are in PDF format, so the first step is to preprocess them - partition to extract text, and chunk them.\n",
        "\n",
        "Unstructured simplifies the data processing from any source to any destination with its ingestion pipeline. We'll configure the pipeline with a local source connector to read the PDFs from a local directory and a local destination connector to store the processed data. The Unstructured processing pipeline can be assembled from a number of configurations:\n",
        "\n",
        "* `ProcessorConfig` describes general behavior such as logs verbosity, number of processes, etc.\n",
        "* `LocalIndexerConfig`, `LocalDownloaderConfig`, and `LocalConnectionConfig` control data ingestion from a local source, you only need to provide a path to your local directory with PDFs here.\n",
        "* `PartitionerConfig`: use it to supply your credentials for the Unstructured Serverless API, and customize the partitioning behavior, e.g. what partitioning strategy to use, whether to exclude some types of metadata, etc. In this case, we use the fast strategy to partition the files, as the PDFs are not complex and contain text only.\n",
        "* `ChunkerConfig`: after partitioning we will chunk the documents into meaningful sized chunks that are not exceeding the input size of all the embedding models we'll be evaluating.\n",
        "* `LocalUploaderConfig`: specify a local directory to load the processed files into.   "
      ],
      "id": "356e8798d2b085ff"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T13:05:17.076487Z",
          "start_time": "2024-08-19T13:05:16.031165Z"
        },
        "id": "35ddea0bf99f8466"
      },
      "cell_type": "code",
      "source": [
        "Pipeline.from_configs(\n",
        "    context=ProcessorConfig(\n",
        "        verbose=True,\n",
        "        tqdm=True,\n",
        "        num_processes=5\n",
        "    ),\n",
        "    indexer_config=LocalIndexerConfig(input_path=\"PDFS\"),\n",
        "    downloader_config=LocalDownloaderConfig(),\n",
        "    source_connection_config=LocalConnectionConfig(),\n",
        "    partitioner_config=PartitionerConfig(\n",
        "        partition_by_api=True,\n",
        "        api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
        "        partition_endpoint=os.getenv(\"UNSTRUCTURED_URL\"),\n",
        "        strategy=\"fast\", # for complex image-based PDFs replace this with \"hi_res\"\n",
        "        additional_partition_args={\n",
        "                \"split_pdf_page\": True,\n",
        "                \"split_pdf_allow_failed\": True,\n",
        "                \"split_pdf_concurrency_level\": 15\n",
        "            }\n",
        "        ),\n",
        "    chunker_config=ChunkerConfig(\n",
        "        chunking_strategy=\"by_title\",\n",
        "        chunk_max_characters=1500,\n",
        "        chunk_overlap = 150,\n",
        "        ),\n",
        "    uploader_config=LocalUploaderConfig(output_dir=\"local-ingest-output\")\n",
        ").run()"
      ],
      "id": "35ddea0bf99f8466",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1224844a4b995677"
      },
      "cell_type": "markdown",
      "source": [
        "Since we've selected the `fast` strategy for this preprocessing pipeline, it should take only a minute or two to preprocess the two large PDFs. You can learn more about the partitioning strategies Unstructured offers in the [documentation](https://docs.unstructured.io/api-reference/api-services/partitioning).\n",
        "\n",
        "Once the pipeline finishes running, you'll find two `*.json` documents, one per original PDF file, in the `local-ingest-output` directory. These files contain the chunks extracted from the original documents that we'll use in the next step to build an evaluation dataset."
      ],
      "id": "1224844a4b995677"
    },
    {
      "metadata": {
        "id": "c7eddee18a6d0294"
      },
      "cell_type": "markdown",
      "source": [
        "## Create an evaluation dataset"
      ],
      "id": "c7eddee18a6d0294"
    },
    {
      "metadata": {
        "id": "44cf59e3058987f1"
      },
      "cell_type": "markdown",
      "source": [
        "Because we don't have real user queries for our 10-K data, we'll create a synthetic evaluation dataset, which is always better than nothing, and you can always wean yourself off of synthetic evaluation dataset once you have actual user queries.\n",
        "\n",
        "To create the dataset we will generate question-answer pairs for each of the document chunks. First, let's load all the processed files from the output directory:"
      ],
      "id": "44cf59e3058987f1"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T16:27:25.722074Z",
          "start_time": "2024-08-20T16:27:25.718219Z"
        },
        "id": "6f0d1c42f564ca25"
      },
      "cell_type": "code",
      "source": [
        "def load_processed_files(directory_path):\n",
        "    \"\"\"\n",
        "    Reads all preprocessed data from JSON files in the given directory and returns elements as a list\n",
        "    \"\"\"\n",
        "    elements = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith('.json'):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            try:\n",
        "                elements.extend(elements_to_dicts(elements_from_json(filename=file_path)))\n",
        "            except IOError:\n",
        "                print(f\"Error: Could not read file {filename}.\")\n",
        "\n",
        "    return elements"
      ],
      "id": "6f0d1c42f564ca25",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T14:47:32.797486Z",
          "start_time": "2024-08-20T14:47:31.202038Z"
        },
        "id": "51268dc2bca7f007",
        "outputId": "3962d220-3884-4a1c-b9d6-a0d32f86c9c3"
      },
      "cell_type": "code",
      "source": [
        "elements = load_processed_files(\"local-ingest-output\")\n",
        "\n",
        "len(elements)"
      ],
      "id": "51268dc2bca7f007",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1082"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "912c5440ec28089e"
      },
      "cell_type": "markdown",
      "source": [
        "Let's add a helper function that will parse string LLM responses into a dictionary, we'll also add `context` (chunk content) and `chunk_id` of the chunk the question is based on, so that we could later see whether we retrieve this chunk or not:"
      ],
      "id": "912c5440ec28089e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T14:47:36.037691Z",
          "start_time": "2024-08-20T14:47:36.033895Z"
        },
        "id": "496514596d07c6de"
      },
      "cell_type": "code",
      "source": [
        "def convert_qa_string_to_dict(input_string, chunk_id, chunk_text):\n",
        "    \"\"\"\n",
        "    Converts a string response from an LLM to a Python dictionary with question-answer-context entries.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = json.loads(input_string)\n",
        "        questions = result[\"questions\"]\n",
        "        for question in questions:\n",
        "            question['id'] = chunk_id\n",
        "            question['context'] = chunk_text\n",
        "        return questions\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return []\n"
      ],
      "id": "496514596d07c6de",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b3550b0d8a0b0b1e"
      },
      "cell_type": "markdown",
      "source": [
        "To create the synthetic evaluation dataset, we'll go over the chunks, and for each chunk we'll prompt the local `llama3.1:8b` model to generate two question/answer pairs.  To use Claude instead, use the commented out section."
      ],
      "id": "b3550b0d8a0b0b1e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T14:55:54.678600Z",
          "start_time": "2024-08-20T14:55:54.674113Z"
        },
        "id": "c2b5045385f88315"
      },
      "cell_type": "code",
      "source": [
        "def generate_chunk_qa_pairs(element):\n",
        "    \"\"\"\n",
        "    Uses a local LLM to generate two question-answer pairs for an individual chunk, then\n",
        "    parses the string response to a Python dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = \"\"\"\n",
        "    You are an assistant specialized in RAG tasks. \\n\n",
        "    The task is the following: given a document chunk, you will have to\n",
        "    generate questions that can be asked by a user to retrieve information from\n",
        "    a large documentary corpus. \\n\n",
        "    The question should be relevant to the chunk, and should not be too specific\n",
        "    or too general. The question should be about the subject of the chunk, and\n",
        "    the answer needs to be found in the chunk. \\n\n",
        "\n",
        "    Remember that the question is asked by a user to get some information from a\n",
        "    large documentary corpus. \\n\n",
        "\n",
        "    Generate a question that could be asked by a user without knowing the existence and the content of the corpus. \\n\n",
        "    Also generate the answer to the question, which should be found in the\n",
        "    document chunk.  \\n\n",
        "    Generate TWO pairs of questions and answers per chunk in a\n",
        "    dictionary with the following format, your answer should ONLY contain this dictionary, NOTHING ELSE: \\n\n",
        "    {\n",
        "        \"questions\": [\n",
        "            {\n",
        "                \"question\": \"XXXXXX\",\n",
        "                \"answer\": \"YYYYYY\",\n",
        "            },\n",
        "            {\n",
        "                \"question\": \"XXXXXX\",\n",
        "                \"answer\": \"YYYYYY\",\n",
        "            },\n",
        "        ]\n",
        "    }\n",
        "    where XXXXXX is the question, YYYYYY is the corresponding answers that could be as long as needed. \\n\n",
        "    Note: If there are no questions to ask about the chunk, return an empty list.\n",
        "    Focus on making relevant questions concerning the page. \\n\n",
        "    Here is the chunk: \\n\n",
        "\"\"\"\n",
        "\n",
        "    response = ollama.generate('llama3.1:8b', prompt + element['text'])\n",
        "    return convert_qa_string_to_dict(response['response'], element['element_id'], element['text'])\n",
        "\n",
        "    # replace with the following if you want to switch to Claude3.5-sonnet\n",
        "    # client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
        "    # response = client.messages.create(\n",
        "    #     model=\"claude-3-5-sonnet-20240620\",\n",
        "    #     max_tokens=1024,\n",
        "    #     messages=[\n",
        "    #         {\"role\": \"user\", \"content\": prompt + element['text']}\n",
        "    #     ]\n",
        "    # )\n",
        "    # return convert_qa_string_to_dict(response.content[0].text, element['element_id'], element['text'])\n"
      ],
      "id": "c2b5045385f88315",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T14:47:44.681793Z",
          "start_time": "2024-08-20T14:47:44.679185Z"
        },
        "id": "507df1f41e110d3"
      },
      "cell_type": "code",
      "source": [
        "def generate_qa_pairs_dataset(elements):\n",
        "    \"\"\"\n",
        "    Creates a dataset of question-answer-context pairs from a dictionary with elements.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = []\n",
        "    for el in elements:\n",
        "        dataset.extend(generate_chunk_qa_pairs(el))\n",
        "    return dataset"
      ],
      "id": "507df1f41e110d3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "66c9df7a12fdb625"
      },
      "cell_type": "markdown",
      "source": [
        "Running the following cell can take a long time depending on your hardware, a model you use, how large your documents are and how many of them you have.\n",
        "You may also see a few JSON parsing errors, this isnâ€™t a big issue in this case, we're simply skipping malformatted question-answer pairs, and still have plenty for the dataset.\n"
      ],
      "id": "66c9df7a12fdb625"
    },
    {
      "metadata": {
        "id": "c38c11c82f693285"
      },
      "cell_type": "code",
      "source": [
        "eval_dataset = generate_qa_pairs_dataset(elements)"
      ],
      "id": "c38c11c82f693285",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "505a4be87f773246"
      },
      "cell_type": "markdown",
      "source": [
        "Once you have generated the dataset, save the results into a local `*.csv` file."
      ],
      "id": "505a4be87f773246"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T22:26:07.955172Z",
          "start_time": "2024-08-19T22:26:07.906098Z"
        },
        "id": "92c12c172c980dec",
        "outputId": "a51ea927-286d-4848-d4de-e653d0fbb008"
      },
      "cell_type": "code",
      "source": [
        "def save_dataset_as_csv(dict_list, output_file):\n",
        "    \"\"\"\n",
        "    Saves a list of dictionaries with QA pairs as a CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame(dict_list)\n",
        "    df = df[df['question'].notna()]\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"DataFrame saved to {output_file}\")\n",
        "\n",
        "save_dataset_as_csv(eval_dataset, \"qa_pairs_dataset.csv\")"
      ],
      "id": "92c12c172c980dec",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame saved to qa_pairs_dataset.csv\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "851661e21ca71b26"
      },
      "cell_type": "markdown",
      "source": [
        "Here's what the dataset looks like."
      ],
      "id": "851661e21ca71b26"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T16:18:36.523776Z",
          "start_time": "2024-08-20T16:18:36.499301Z"
        },
        "id": "9cd53ae547162c58",
        "outputId": "1013de77-1e2b-4de1-87f5-06ba65a815b8"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"qa_pairs_dataset.csv\")\n",
        "df.head()"
      ],
      "id": "9cd53ae547162c58",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the name of the corporation that submi...   \n",
              "1  What type of securities are registered with th...   \n",
              "2  What is the name of the stock exchange mention...   \n",
              "3  Is the registrant a well-known seasoned issuer...   \n",
              "4     What is the category of the first item listed?   \n",
              "\n",
              "                                              answer  \\\n",
              "0                            Exxon Mobil Corporation   \n",
              "1  Common Stock, without par value 0.142% Notes d...   \n",
              "2                            New York Stock Exchange   \n",
              "3                                                Yes   \n",
              "4                            Large accelerated filer   \n",
              "\n",
              "                                 id  \\\n",
              "0  10982f6a0008dfa086cd91f90df940e7   \n",
              "1  10982f6a0008dfa086cd91f90df940e7   \n",
              "2  f9adc4b9c1da74f14e91b510533ef9d6   \n",
              "3  f9adc4b9c1da74f14e91b510533ef9d6   \n",
              "4  5399d22de8c81de1534b9bb3a067a3f1   \n",
              "\n",
              "                                             context  \n",
              "0  2023\\n\\nUNITED STATES SECURITIES AND EXCHANGE ...  \n",
              "1  2023\\n\\nUNITED STATES SECURITIES AND EXCHANGE ...  \n",
              "2  XOM XOM24B XOM28 XOM32 XOM39A\\n\\nNew York Stoc...  \n",
              "3  XOM XOM24B XOM28 XOM32 XOM39A\\n\\nNew York Stoc...  \n",
              "4  Large accelerated filer Non-accelerated filer\\...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the name of the corporation that submi...</td>\n",
              "      <td>Exxon Mobil Corporation</td>\n",
              "      <td>10982f6a0008dfa086cd91f90df940e7</td>\n",
              "      <td>2023\\n\\nUNITED STATES SECURITIES AND EXCHANGE ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What type of securities are registered with th...</td>\n",
              "      <td>Common Stock, without par value 0.142% Notes d...</td>\n",
              "      <td>10982f6a0008dfa086cd91f90df940e7</td>\n",
              "      <td>2023\\n\\nUNITED STATES SECURITIES AND EXCHANGE ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the name of the stock exchange mention...</td>\n",
              "      <td>New York Stock Exchange</td>\n",
              "      <td>f9adc4b9c1da74f14e91b510533ef9d6</td>\n",
              "      <td>XOM XOM24B XOM28 XOM32 XOM39A\\n\\nNew York Stoc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Is the registrant a well-known seasoned issuer...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>f9adc4b9c1da74f14e91b510533ef9d6</td>\n",
              "      <td>XOM XOM24B XOM28 XOM32 XOM39A\\n\\nNew York Stoc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the category of the first item listed?</td>\n",
              "      <td>Large accelerated filer</td>\n",
              "      <td>5399d22de8c81de1534b9bb3a067a3f1</td>\n",
              "      <td>Large accelerated filer Non-accelerated filer\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5c850fe4a262e68"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up retrievers and collect responses to questions"
      ],
      "id": "5c850fe4a262e68"
    },
    {
      "metadata": {
        "id": "976ce0e0fb402ac9"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have our synthetic dataset and processed documents, it's time to put our embedding models to the test. For each model, we'll:\n",
        "* Set up a retriever with chunks using LangChain and ChromaDB: We'll initialize a retriever using Chroma and the chosen embedding model.\n",
        "* Load the questions from our synthetic dataset.\n",
        "* Collect retrieval results: We'll use the retriever to find N most relevant document chunks for each question in the evaluation dataset.\n",
        "* We'll store the retrieved document IDs along with the corresponding questions.\n",
        "\n",
        "The following function does just that:"
      ],
      "id": "976ce0e0fb402ac9"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T16:27:31.102919Z",
          "start_time": "2024-08-20T16:27:31.097203Z"
        },
        "id": "ec98d8630ec0e7d3"
      },
      "cell_type": "code",
      "source": [
        "def setup_and_query_rag(embedding_model, documents, eval_dataset, output_directory, n_to_retrieve=10):\n",
        "\n",
        "    elements = load_processed_files(documents)\n",
        "    staged_elements = dict_to_elements(elements)\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    for element in staged_elements:\n",
        "        metadata = element.metadata.to_dict()\n",
        "        metadata['element_id'] = element._element_id\n",
        "        del metadata['orig_elements']\n",
        "        documents.append(Document(page_content=element.text, metadata=metadata))\n",
        "\n",
        "    documents = chromautils.filter_complex_metadata(documents)\n",
        "    db = Chroma.from_documents(documents, HuggingFaceEmbeddings(model_name=embedding_model))\n",
        "    retriever =  db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": n_to_retrieve})\n",
        "\n",
        "    df = pd.read_csv(eval_dataset)\n",
        "    df = df[df['question'].notna()]\n",
        "    questions = df[\"question\"].to_list()\n",
        "\n",
        "    results = []\n",
        "    for question in questions:\n",
        "        try:\n",
        "            retrieved_documents = retriever.invoke(question)\n",
        "            retrieved_ids = [doc.metadata['element_id'] for doc in retrieved_documents]\n",
        "            results.append({\"question\": question, \"retrieved_ids\": retrieved_ids})\n",
        "        except:\n",
        "            print(f\"Skipped question: {question}\")\n",
        "\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    file_path = os.path.join(output_directory, f\"{embedding_model.replace('/', '@')}-{n_to_retrieve}.csv\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(f\"DataFrame saved to {file_path}\")\n",
        "    db.delete_collection()"
      ],
      "id": "ec98d8630ec0e7d3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "391cfed51c37ba84"
      },
      "cell_type": "markdown",
      "source": [
        "Let's collect 10 retrieval results for each question from each retriever."
      ],
      "id": "391cfed51c37ba84"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T14:36:15.396798Z",
          "start_time": "2024-08-20T14:31:01.325229Z"
        },
        "id": "3595ac4a9ef6226a",
        "outputId": "24745c79-f8a1-4cb0-e380-a2896ae3d30f"
      },
      "cell_type": "code",
      "source": [
        "models = [\"BAAI/bge-large-en-v1.5\", \"mukaj/fin-mpnet-base\", \"Snowflake/snowflake-arctic-embed-l\"]\n",
        "\n",
        "for model in models:\n",
        "    setup_and_query_rag(model, \"local-ingest-output\", \"qa_pairs_dataset.csv\", \"retriever_results\")"
      ],
      "id": "3595ac4a9ef6226a",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame saved to retriever_results/BAAI@bge-large-en-v1.5-10.csv\n",
            "DataFrame saved to retriever_results/mukaj@fin-mpnet-base-10.csv\n",
            "DataFrame saved to retriever_results/Snowflake@snowflake-arctic-embed-l-10.csv\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fe037bffafcba4c3"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let's collect 100 retrieval results for each question from each retriever."
      ],
      "id": "fe037bffafcba4c3"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T16:33:27.766498Z",
          "start_time": "2024-08-20T16:27:55.536512Z"
        },
        "id": "c461e89c418b02b1",
        "outputId": "f5741cea-093d-4593-b4f7-412f3e94ed7f"
      },
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "    setup_and_query_rag(model, \"local-ingest-output\", \"qa_pairs_dataset.csv\", \"retriever_results_100\", 100)"
      ],
      "id": "c461e89c418b02b1",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/Users/mk/Code/eval_data/eval_data/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame saved to retriever_results_100/BAAI@bge-large-en-v1.5-100.csv\n",
            "DataFrame saved to retriever_results_100/mukaj@fin-mpnet-base-100.csv\n",
            "DataFrame saved to retriever_results_100/Snowflake@snowflake-arctic-embed-l-100.csv\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "db8b95cf5bde49ec"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate the metrics and compare the results"
      ],
      "id": "db8b95cf5bde49ec"
    },
    {
      "metadata": {
        "id": "a8741862335da8b2"
      },
      "cell_type": "markdown",
      "source": [
        "Once you have the results from each of the retrievers, let's calculate some metrics.\n",
        "In this example, we'll use two metrics: Recall, and MRR.\n",
        "\n",
        "Since the evaluation dataset has one relevant chunk per question, the average Recall will tell us how often this chunk was retrieved _at all_ in the K retrieved documents. The value of 1 would mean that we retrieved the relevant chunk for every question (without taking into account its position in the list of retrieved chunks), the value of 0 would mean that the relevant chunk was never retrieved for any question. The higher the average recall, the better.\n",
        "\n",
        "The average MRR (Mean reciprocal rank) will tell us the average position of the relevant chunk in the list of retrieved chunks, e.g. mrr = 1 would mean it was always the first result, mrr = 1/2 would mean it was second, etc. The higher the average MRR, the better.  "
      ],
      "id": "a8741862335da8b2"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T16:36:25.939319Z",
          "start_time": "2024-08-20T16:36:25.934859Z"
        },
        "id": "4c4117ff0872692e"
      },
      "cell_type": "code",
      "source": [
        "def calculate_retrieval_metrics(evaluation_data: pd.DataFrame, retrieval_results: pd.DataFrame, top_k=10):\n",
        "    eval_list = evaluation_data.to_dict('records')\n",
        "    retrieval_list = retrieval_results.to_dict('records')\n",
        "    recall = []\n",
        "    ranks = []\n",
        "\n",
        "    for item in retrieval_list:\n",
        "        question = item[\"question\"]\n",
        "        retrieved_ids = eval(item[\"retrieved_ids\"])[:top_k]\n",
        "\n",
        "        for eval_point in eval_list:\n",
        "            if eval_point['question'] == question:\n",
        "                correct_id = eval_point[\"id\"]\n",
        "\n",
        "        if correct_id in retrieved_ids:\n",
        "            recall.append(1)\n",
        "            rank = retrieved_ids.index(correct_id) + 1\n",
        "            ranks.append(1 / rank)\n",
        "        else:\n",
        "            recall.append(0)\n",
        "            ranks.append(0)\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_recall = sum(recall) / len(retrieval_list)\n",
        "    mrr = sum(ranks) / len(retrieval_list)\n",
        "    metrics = {\n",
        "        'Recall': avg_recall,\n",
        "        'MRR': mrr,\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "id": "4c4117ff0872692e",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ee4cb70de04f665c"
      },
      "cell_type": "markdown",
      "source": [
        "Let's calculate the metrics for 10 retrieved results:"
      ],
      "id": "ee4cb70de04f665c"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T22:35:27.268476Z",
          "start_time": "2024-08-19T22:35:26.812130Z"
        },
        "id": "a86c2e927eafa1cb"
      },
      "cell_type": "code",
      "source": [
        "eval_dataset = pd.read_csv(\"qa_pairs_dataset.csv\")\n",
        "directory_with_retrieval_results = \"retriever_results\"\n",
        "k = 10\n",
        "all_metrics = dict()\n",
        "\n",
        "for filename in os.listdir(directory_with_retrieval_results):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory_with_retrieval_results, filename)\n",
        "        try:\n",
        "            model_name = filename[:-4].rsplit('-', 1)[0].replace('@', '/')\n",
        "            retrieval_results = pd.read_csv(file_path)\n",
        "            all_metrics[model_name] = calculate_retrieval_metrics(eval_dataset, retrieval_results, top_k=k)\n",
        "        except IOError:\n",
        "            print(f\"Error: Could not read file {filename}.\")"
      ],
      "id": "a86c2e927eafa1cb",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T22:35:28.546271Z",
          "start_time": "2024-08-19T22:35:28.542959Z"
        },
        "id": "8ce9f2cca713964a",
        "outputId": "6c452af5-3b1e-491c-e5d7-89ee38037768"
      },
      "cell_type": "code",
      "source": [
        "all_metrics"
      ],
      "id": "8ce9f2cca713964a",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Snowflake/snowflake-arctic-embed-l': {'Recall': 0.3502610346464167,\n",
              "  'MRR': 0.20764157268666042},\n",
              " 'BAAI/bge-large-en-v1.5': {'Recall': 0.8794494542002848,\n",
              "  'MRR': 0.6415374677002579},\n",
              " 'mukaj/fin-mpnet-base': {'Recall': 0.8239202657807309,\n",
              "  'MRR': 0.5528548074822408}}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-20T16:36:44.153570Z",
          "start_time": "2024-08-20T16:36:43.064814Z"
        },
        "id": "57eddccd249fff12",
        "outputId": "81666901-33fd-4882-a790-fb5a3ba641ab"
      },
      "cell_type": "code",
      "source": [
        "eval_dataset = pd.read_csv(\"qa_pairs_dataset.csv\")\n",
        "directory_with_retrieval_results = \"retriever_results_100\"\n",
        "k = 100\n",
        "all_metrics_100 = dict()\n",
        "\n",
        "for filename in os.listdir(directory_with_retrieval_results):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory_with_retrieval_results, filename)\n",
        "        try:\n",
        "            model_name = filename[:-4].rsplit('-', 1)[0].replace('@', '/')\n",
        "            retrieval_results = pd.read_csv(file_path)\n",
        "            all_metrics_100[model_name] = calculate_retrieval_metrics(eval_dataset, retrieval_results, top_k=k)\n",
        "        except IOError:\n",
        "            print(f\"Error: Could not read file {filename}.\")\n",
        "\n",
        "all_metrics_100"
      ],
      "id": "57eddccd249fff12",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Snowflake/snowflake-arctic-embed-l': {'Recall': 0.7446606549596583,\n",
              "  'MRR': 0.24060553153830655},\n",
              " 'BAAI/bge-large-en-v1.5': {'Recall': 0.9905078310393926,\n",
              "  'MRR': 0.6660323162790893},\n",
              " 'mukaj/fin-mpnet-base': {'Recall': 0.9857617465590888,\n",
              "  'MRR': 0.5725198750014092}}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ae1c5023520fbfca"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see, for 10 retrieved documents, `'BAAI/bge-large-en-v1.5'` has both the highest recall and the highest MRR, so it is a clear winner! However, I wouldn't disregard `'mukaj/fin-mpnet-base'`. It is a close second, and it's even closer to the top when we retrieve 100 results instead of 10. It's also x3 times smaller than `'BAAI/bge-large-en-v1.5'`, so it might be a good choice still, especially if you add a reranker step to your RAG. However, for simplicity, here we'll pick the model that had the best recall on 10 retrievals."
      ],
      "id": "ae1c5023520fbfca"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T22:38:53.562072Z",
          "start_time": "2024-08-19T22:38:53.559104Z"
        },
        "id": "5477341944e21cd",
        "outputId": "0dceed1f-97dc-442d-874b-bb688cc4f1c0"
      },
      "cell_type": "code",
      "source": [
        "model_with_max_recall = max(all_metrics, key=lambda k: all_metrics[k]['Recall'])\n",
        "model_with_max_recall"
      ],
      "id": "5477341944e21cd",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'BAAI/bge-large-en-v1.5'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3687dcc7f0453579"
      },
      "cell_type": "markdown",
      "source": [
        "## Complete the preprocessing pipeline with an embedding and upload steps\n",
        "\n",
        "Once we have or choice of the best embedding model, we can simply add a new embedding step to the existing pipeline, and run the pipeline one more time. The results of partitioning and chunking are already cached, so by adding an embedding step to the pipeline, it will pick up at the embedding step, and won't re-process the documents from scratch.\n",
        "\n",
        "Here we change the destination to a different local directory, but you can set up a vector store as a destination instead.\n",
        "Find out how to configure your favorite vector store as a destination connector in Unstructured [documentation](https://docs.unstructured.io/api-reference/ingest/destination-connector/overview)."
      ],
      "id": "3687dcc7f0453579"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T22:41:31.027567Z",
          "start_time": "2024-08-19T22:39:10.868311Z"
        },
        "id": "17da3022a3108977",
        "outputId": "d4c8253c-ba43-4849-841d-b3aaf045a7e0"
      },
      "cell_type": "code",
      "source": [
        "Pipeline.from_configs(\n",
        "    context=ProcessorConfig(\n",
        "        verbose=True,\n",
        "        tqdm=True,\n",
        "        num_processes=20,\n",
        "    ),\n",
        "    indexer_config=LocalIndexerConfig(input_path=\"PDFS\"),\n",
        "    downloader_config=LocalDownloaderConfig(),\n",
        "    source_connection_config=LocalConnectionConfig(),\n",
        "    partitioner_config=PartitionerConfig(\n",
        "        partition_by_api=True,\n",
        "        api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
        "        partition_endpoint=os.getenv(\"UNSTRUCTURED_URL\"),\n",
        "        strategy=\"fast\",\n",
        "        additional_partition_args={\n",
        "                \"split_pdf_page\": True,\n",
        "                \"split_pdf_allow_failed\": True,\n",
        "                \"split_pdf_concurrency_level\": 15\n",
        "            }\n",
        "        ),\n",
        "    chunker_config=ChunkerConfig(\n",
        "        chunking_strategy=\"by_title\",\n",
        "        chunk_max_characters=1500,\n",
        "        chunk_overlap = 150,\n",
        "        ),\n",
        "    embedder_config=EmbedderConfig(\n",
        "        embedding_provider=\"langchain-huggingface\",\n",
        "        embedding_model_name=model_with_max_recall, # use the model with the highest recall\n",
        "    ),\n",
        "    # We're changing the output location here, but you can switch to a vector store as a destination\n",
        "    uploader_config=LocalUploaderConfig(output_dir=\"outputs-with-embeddings\")\n",
        ").run()"
      ],
      "id": "17da3022a3108977",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-19 18:39:10,872 MainProcess INFO     Created index with configs: {\"input_path\": \"PDFS\", \"recursive\": false}, connection configs: {\"access_config\": {}}\n",
            "2024-08-19 18:39:10,873 MainProcess INFO     Created download with configs: {\"download_dir\": null}, connection configs: {\"access_config\": {}}\n",
            "2024-08-19 18:39:10,874 MainProcess INFO     Created partition with configs: {\"strategy\": \"fast\", \"ocr_languages\": null, \"encoding\": null, \"additional_partition_args\": {\"split_pdf_page\": true, \"split_pdf_allow_failed\": true, \"split_pdf_concurrency_level\": 15}, \"skip_infer_table_types\": null, \"fields_include\": [\"element_id\", \"text\", \"type\", \"metadata\", \"embeddings\"], \"flatten_metadata\": false, \"metadata_exclude\": [], \"metadata_include\": [], \"partition_endpoint\": \"https://api.unstructuredapp.io/general/v0/general\", \"partition_by_api\": true, \"api_key\": \"*******\", \"hi_res_model_name\": null}\n",
            "2024-08-19 18:39:10,875 MainProcess INFO     Created chunk with configs: {\"chunking_strategy\": \"by_title\", \"chunking_endpoint\": \"https://api.unstructured.io/general/v0/general\", \"chunk_by_api\": false, \"chunk_api_key\": null, \"chunk_combine_text_under_n_chars\": null, \"chunk_include_orig_elements\": null, \"chunk_max_characters\": 1500, \"chunk_multipage_sections\": null, \"chunk_new_after_n_chars\": null, \"chunk_overlap\": 150, \"chunk_overlap_all\": null}\n",
            "2024-08-19 18:39:10,875 MainProcess INFO     Created embed with configs: {\"embedding_provider\": \"langchain-huggingface\", \"embedding_api_key\": null, \"embedding_model_name\": \"BAAI/bge-large-en-v1.5\", \"embedding_aws_access_key_id\": null, \"embedding_aws_secret_access_key\": null, \"embedding_aws_region\": null}\n",
            "2024-08-19 18:39:13,251 MainProcess INFO     Created upload with configs: {\"output_dir\": \"outputs-with-embeddings\"}, connection configs: {\"access_config\": {}}\n",
            "2024-08-19 18:39:13,253 MainProcess INFO     Running local pipline: index (LocalIndexer) -> download (LocalDownloader) -> partition (fast) -> chunk (by_title) -> embed (langchain-huggingface) -> upload (LocalUploader) with configs: {\"reprocess\": false, \"verbose\": true, \"tqdm\": true, \"work_dir\": \"/Users/mk/.cache/unstructured/ingest/pipeline\", \"num_processes\": 20, \"max_connections\": null, \"raise_on_error\": false, \"disable_parallelism\": false, \"preserve_downloads\": false, \"download_only\": false, \"max_docs\": null, \"re_download\": false, \"uncompress\": false, \"status\": {}, \"semaphore\": null}\n",
            "2024-08-19 18:39:13,304 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"1. Walmart, Inc.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\", \"rel_path\": \"1. Walmart, Inc.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\"}, \"date_created\": \"1722017589.8771877\", \"date_modified\": \"1722017589.8771877\", \"date_processed\": \"1724107153.30389\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 1742387}, \"additional_metadata\": {}, \"reprocess\": false}\n",
            "2024-08-19 18:39:13,306 MainProcess DEBUG    Generated file data: {\"identifier\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"connector_type\": \"local\", \"source_identifiers\": {\"filename\": \"2.Exxon Mobil Corporation.pdf\", \"fullpath\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\", \"rel_path\": \"2.Exxon Mobil Corporation.pdf\"}, \"doc_type\": \"file\", \"metadata\": {\"url\": null, \"version\": null, \"record_locator\": {\"path\": \"/Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\"}, \"date_created\": \"1722017631.1280465\", \"date_modified\": \"1722017631.1280465\", \"date_processed\": \"1724107153.306206\", \"permissions_data\": [{\"mode\": 33188}], \"filesize_bytes\": 13891749}, \"additional_metadata\": {}, \"reprocess\": false}\n",
            "2024-08-19 18:39:13,307 MainProcess INFO     Calling DownloadStep with 2 docs\n",
            "2024-08-19 18:39:13,307 MainProcess INFO     processing content async\n",
            "2024-08-19 18:39:13,308 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
            "download:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 18:39:13,312 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/2.Exxon Mobil Corporation.pdf\n",
            "2024-08-19 18:39:13,313 MainProcess DEBUG    Skipping download, file already exists locally: /Users/mk/Code/eval_data/eval_data/PDFS/1. Walmart, Inc.pdf\n",
            "download: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 560.70it/s]\n",
            "2024-08-19 18:39:13,314 MainProcess INFO     DownloadStep [cls] took 0.006888866424560547 seconds\n",
            "2024-08-19 18:39:13,314 MainProcess INFO     Calling PartitionStep with 2 docs\n",
            "2024-08-19 18:39:13,314 MainProcess INFO     processing content async\n",
            "2024-08-19 18:39:13,314 MainProcess WARNING  async code being run in dedicated thread pool to not conflict with existing event loop: <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
            "partition:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 18:39:13,317 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/c54e87910a8f.json\n",
            "2024-08-19 18:39:13,318 MainProcess DEBUG    Skipping partitioning, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/partition/8174f7931121.json\n",
            "partition: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 685.62it/s]\n",
            "2024-08-19 18:39:13,319 MainProcess INFO     PartitionStep [cls] took 0.004873037338256836 seconds\n",
            "2024-08-19 18:39:13,319 MainProcess INFO     Calling ChunkStep with 2 docs\n",
            "2024-08-19 18:39:13,319 MainProcess INFO     processing content across processes\n",
            "chunk:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 18:39:14,986 SpawnPoolWorker-10 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/bec3cbee1521.json\n",
            "chunk:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.35s/it]2024-08-19 18:39:14,991 SpawnPoolWorker-11 DEBUG    Skipping chunking, output already exists: /Users/mk/.cache/unstructured/ingest/pipeline/chunk/3c703e122845.json\n",
            "chunk: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.47it/s]\n",
            "2024-08-19 18:39:15,032 MainProcess INFO     ChunkStep [cls] took 1.7130928039550781 seconds\n",
            "2024-08-19 18:39:15,033 MainProcess INFO     Calling EmbedStep with 2 docs\n",
            "2024-08-19 18:39:15,033 MainProcess INFO     processing content across processes\n",
            "embed:   0%|          | 0/2 [00:00<?, ?it/s]2024-08-19 18:41:08,764 SpawnPoolWorker-23 DEBUG    Writing embedded output to: /Users/mk/.cache/unstructured/ingest/pipeline/embed/ceb56fa1fb24.json\n",
            "embed:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:53<01:53, 113.34s/it]2024-08-19 18:41:30,401 SpawnPoolWorker-22 DEBUG    Writing embedded output to: /Users/mk/.cache/unstructured/ingest/pipeline/embed/abe3b0334474.json\n",
            "embed: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:15<00:00, 67.53s/it] \n",
            "2024-08-19 18:41:31,008 MainProcess INFO     EmbedStep [cls] took 135.9749517440796 seconds\n",
            "2024-08-19 18:41:31,008 MainProcess INFO     Calling UploadStep with 2 docs\n",
            "2024-08-19 18:41:31,011 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/embed/ceb56fa1fb24.json to /Users/mk/Code/eval_data/eval_data/outputs-with-embeddings/2.Exxon Mobil Corporation.pdf.json\n",
            "2024-08-19 18:41:31,015 MainProcess DEBUG    copying file from /Users/mk/.cache/unstructured/ingest/pipeline/embed/abe3b0334474.json to /Users/mk/Code/eval_data/eval_data/outputs-with-embeddings/1. Walmart, Inc.pdf.json\n",
            "2024-08-19 18:41:31,019 MainProcess INFO     UploadStep [cls] took 0.010811090469360352 seconds\n",
            "2024-08-19 18:41:31,020 MainProcess INFO     Finished ingest process in 137.76718425750732s\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}